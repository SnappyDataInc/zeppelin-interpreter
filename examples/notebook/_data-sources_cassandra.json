{"paragraphs":[{"text":"%snappydata\n\nprintln(\"%html <h3> Specify Dataset name(registered as External table), along with cassandra Host, port, keyspace, table to read from </h3>\")\nprintln(\"%html <h4> <a href='https://github.com/datastax/spark-cassandra-connector/blob/master/doc/1_connecting.md' target='_blank'>Read description of all config options</a> </h4>\")\n","dateUpdated":"2018-12-19T18:32:03+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":false,"results":{},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3> Specify Dataset name(registered as External table), along with cassandra Host, port, keyspace, table to read from </h3>\n"},{"type":"HTML","data":"<h4> <a href='https://github.com/datastax/spark-cassandra-connector/blob/master/doc/1_connecting.md' target='_blank'>Read description of all config options</a> </h4>\n"}]},"apps":[],"jobName":"paragraph_1545223409094_1660879341","id":"20180523-005307_463881908","dateCreated":"2018-12-19T18:13:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:46301"},{"title":"STEP I : Register the dataset using below Cassandra access options","text":"%snappydata\n// this curly brackets around block of code prevents all the REPL output from being displayed ...\n{\nimport com.datastax.spark.connector._\nimport org.apache.spark.sql.cassandra._\nimport com.datastax.spark.connector.cql.CassandraConnectorConf\nimport org.apache.spark.sql.SnappySession\n\nval datasetName = z.input(\"Enter Name for the Dataset:\", \"Invalid-Name\").asInstanceOf[String]\n//val host = z.input(\"Enter any one of Cassandra host IP/Name:\", \"cassandra\")\n//val port = z.input(\"Enter Port:\", \"7077\")\nval keyspace = z.input(\"Enter Keyspace:\", \"test\")\nval table = z.input(\"Enter Table Name:\", \"airlineref\")\n//TODO: Several other important options to be added later ....\n\nz.put(\"datasetName\", datasetName)\n//z.put(\"host\", host)\n//z.put(\"port\", port)\nz.put(\"table\",table)\nz.put(\"keyspace\",keyspace)\nvar connectionString =s\"create external table $datasetName using org.apache.spark.sql.cassandra options (table '$table', keyspace '$keyspace', cluster 'Cluster1'\"\nz.put(\"connectionString\",connectionString)\n}\n","dateUpdated":"2018-12-19T18:32:03+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"results":{},"enabled":true,"tableHide":true},"settings":{"params":{"Name the dataset being created:":"df1","URI:":"mongodb://mongodb-mongodb","Database:":"testdb","Collection:":"airlineRef","Sample size (schema inference):":"1000","Partitioner:":"MongoDefaultPartitioner","Any one Cassandra host IP/Name:":"localhost","Port:":"7077","Keyspace:":"demo1","Cassandra tableName:":"emp","Enter Name for the Dataset:":"ds_cassandra3","Enter Any one of Cassandra host IP/Name:":"localhost","Enter Port:":"7077","Enter Keyspace Name:":"demo1","Enter Cassandra Table Name:":"emp","Enter any one of Cassandra host IP/Name:":"localhost","Enter Keyspace:":"test","Enter Table Name:":"emp"},"forms":{"Enter Name for the Dataset:":{"name":"Enter Name for the Dataset:","displayName":"Enter Name for the Dataset:","type":"input","defaultValue":"Invalid-Name","hidden":false,"$$hashKey":"object:46501"},"Enter Keyspace:":{"name":"Enter Keyspace:","displayName":"Enter Keyspace:","type":"input","defaultValue":"test","hidden":false,"$$hashKey":"object:46502"},"Enter Table Name:":{"name":"Enter Table Name:","displayName":"Enter Table Name:","type":"input","defaultValue":"airlineref","hidden":false,"$$hashKey":"object:46503"}}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1545223409095_1660494593","id":"20180523-001031_1009080999","dateCreated":"2018-12-19T18:13:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:46302"},{"title":"STEP II : Enter additional parameters here...","text":"%snappydata\n{  \nvar flag:Boolean=false\nvar key:String=\" \"\nvar value:String=\" \"\nvar map1:Map[String,String] = Map()\nval datasetName =z.get(\"datasetName\").asInstanceOf[String]\nval table=z.get(\"table\").asInstanceOf[String]\nval keyspace=z.get(\"keyspace\").asInstanceOf[String]\nval ss = new org.apache.spark.sql.SnappySession(snc.sparkContext)\nval parameterString = z.input(\"Enter optional parameters in the form of <key>=<value> separated by comma (,)\", \"\").asInstanceOf[String]\n\nif(parameterString != \"\"){\nfor (returnValue <- parameterString.split(\",\"))\n{\n  for(ret <- returnValue.split(\"=\"))\n    {\n      if(flag==false)\n      {\n        key=ret\n        flag=true\n      }\n      else\n      {\n        value=ret\n        flag=false\n       }\n    }\n map1 += key -> value\n }\n    var stringToConcat = \"\"\n    map1.keys.foreach{ j =>\n    var temp=j\n    var temp2=map1(j)\n     stringToConcat += \",\"+temp+\" \"+\"'\"+temp2+\"'\"\n}\n\nss.sql(s\"drop table if exists $datasetName\")\nval connectionString=z.get(\"connectionString\").asInstanceOf[String]\nval connectionString2=connectionString.concat(stringToConcat)\nval finalConnectionString=connectionString2.concat(\")\")\nss.sql(finalConnectionString)\n}\nelse{\n println(\"No extra parameter\")\n ss.sql(s\"drop table if exists $datasetName\")\n ss.sql(s\"create external table $datasetName using org.apache.spark.sql.cassandra options (table '$table', keyspace '$keyspace', cluster 'Cluster1')\" )\n}\nprintln(s\"External table named $datasetName registered.\")\nprintln(s\"====== Schema for $datasetName ======\")\nss.table(datasetName).printSchema\n}","dateUpdated":"2018-12-19T18:32:03+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"results":{},"enabled":true,"tableHide":true},"settings":{"params":{"Give list of parameters:":" ","Enter optional parameters in the form of <key>=<value> separated by comma (,)":""},"forms":{"Enter optional parameters in the form of <key>=<value> separated by comma (,)":{"name":"Enter optional parameters in the form of <key>=<value> separated by comma (,)","displayName":"Enter optional parameters in the form of <key>=<value> separated by comma (,)","type":"input","defaultValue":"","hidden":false,"$$hashKey":"object:46560"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"No extra parameter\n\nExternal table named ds_cassandra3 registered.\n====== Schema for ds_cassandra3 ======\nroot\n |-- emp_id: integer (nullable = true)\n |-- emp_city: string (nullable = true)\n |-- emp_name: string (nullable = true)\n |-- emp_phone: decimal(38,0) (nullable = true)\n |-- emp_sal: decimal(38,0) (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1545223409096_1658570848","id":"20180927-111933_374698893","dateCreated":"2018-12-19T18:13:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:46303"},{"title":"STEP III : Lets take a look at some sample data","text":"%snappydata\n{\nimport org.apache.spark.sql.SnappySession\n\nval ss = new SnappySession(snc.sparkContext)\nval datasetName = z.get(\"datasetName\")\nval host = z.get(\"host\")\n\nss.sql(s\"set spark.cassandra.connection.host=$host\")\nval DF = ss.table(s\"$datasetName\").limit(10)\nz.show(DF)\n}","dateUpdated":"2018-12-19T18:32:03+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"results":{},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.IllegalArgumentException: Cannot build a cluster without contact points\n  at com.datastax.driver.core.Cluster.checkNotEmpty(Cluster.java:119)\n  at com.datastax.driver.core.Cluster.<init>(Cluster.java:112)\n  at com.datastax.driver.core.Cluster.buildFrom(Cluster.java:178)\n  at com.datastax.driver.core.Cluster$Builder.build(Cluster.java:1299)\n  at com.datastax.spark.connector.cql.DefaultConnectionFactory$.createCluster(CassandraConnectionFactory.scala:131)\n  at com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:159)\n  at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$8.apply(CassandraConnector.scala:154)\n  at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$8.apply(CassandraConnector.scala:154)\n  at com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n  at com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n  at com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n  at com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:79)\n  at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:111)\n  at com.datastax.spark.connector.rdd.partitioner.dht.TokenFactory$.forSystemLocalPartitioner(TokenFactory.scala:98)\n  at org.apache.spark.sql.cassandra.CassandraSourceRelation$.apply(CassandraSourceRelation.scala:272)\n  at org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:56)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:330)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog$$anon$1.load(SnappyStoreHiveCatalog.scala:249)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog$$anon$1.load(SnappyStoreHiveCatalog.scala:221)\n  at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n  at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n  at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n  at com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n  at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n  at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n  at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4880)\n  at com.google.common.cache.LocalCache$LocalLoadingCache.apply(LocalCache.java:4898)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog.getCachedHiveTable(SnappyStoreHiveCatalog.scala:296)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog.lookupRelation(SnappyStoreHiveCatalog.scala:632)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog.lookupRelation(SnappyStoreHiveCatalog.scala:701)\n  at org.apache.spark.sql.SparkSession.table(SparkSession.scala:578)\n  at org.apache.spark.sql.SparkSession.table(SparkSession.scala:574)\n  ... 44 elided\n"}]},"apps":[],"jobName":"paragraph_1545223409097_1658186099","id":"20180523-010008_189512033","dateCreated":"2018-12-19T18:13:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:46304"},{"title":"STEP IV : Cache the data in SnappyData Row Or Column table","text":"%snappydata\n{\nimport org.apache.spark.sql.SnappySession\n\nval ss = new SnappySession(snc.sparkContext)\n\nval datasetName = z.get(\"datasetName\")\nval tabName = datasetName+ \"_cache\"\nval host = z.get(\"host\")\n\nval tableType = z.select(\"SnappyData Table type\", Seq((\"column\" ,\"Column\"),\n                                                    (\"row\",\"row\"))).asInstanceOf[String]\nss.sql(s\"set spark.cassandra.connection.host=$host\")\n\nss.sql(s\"drop table if exists $tabName\")\nss.sql(s\"create table $tabName using $tableType as (select * from $datasetName)\" )\n\nprintln(s\"Created table named $tabName\")\nprintln(\"With row count = \" + ss.table(s\"$tabName\").count )\n}","dateUpdated":"2018-12-19T18:32:03+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"runOnSelectionChange":true,"results":{},"enabled":true,"tableHide":true},"settings":{"params":{"SnappyData Table type":"column"},"forms":{"SnappyData Table type":{"name":"SnappyData Table type","displayName":"SnappyData Table type","type":"select","defaultValue":"","options":[{"value":"column","displayName":"Column","$$hashKey":"object:46656"},{"value":"row","displayName":"row","$$hashKey":"object:46657"}],"hidden":false,"$$hashKey":"object:46648"}}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.IllegalArgumentException: Cannot build a cluster without contact points\n  at com.datastax.driver.core.Cluster.checkNotEmpty(Cluster.java:119)\n  at com.datastax.driver.core.Cluster.<init>(Cluster.java:112)\n  at com.datastax.driver.core.Cluster.buildFrom(Cluster.java:178)\n  at com.datastax.driver.core.Cluster$Builder.build(Cluster.java:1299)\n  at com.datastax.spark.connector.cql.DefaultConnectionFactory$.createCluster(CassandraConnectionFactory.scala:131)\n  at com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:159)\n  at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$8.apply(CassandraConnector.scala:154)\n  at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$8.apply(CassandraConnector.scala:154)\n  at com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n  at com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n  at com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n  at com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:79)\n  at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:111)\n  at com.datastax.spark.connector.rdd.partitioner.dht.TokenFactory$.forSystemLocalPartitioner(TokenFactory.scala:98)\n  at org.apache.spark.sql.cassandra.CassandraSourceRelation$.apply(CassandraSourceRelation.scala:272)\n  at org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:56)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:330)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog$$anon$1.load(SnappyStoreHiveCatalog.scala:249)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog$$anon$1.load(SnappyStoreHiveCatalog.scala:221)\n  at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n  at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n  at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n  at com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n  at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n  at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n  at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4880)\n  at com.google.common.cache.LocalCache$LocalLoadingCache.apply(LocalCache.java:4898)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog.getCachedHiveTable(SnappyStoreHiveCatalog.scala:296)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog.lookupRelation(SnappyStoreHiveCatalog.scala:632)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog.lookupRelation(SnappyStoreHiveCatalog.scala:701)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:457)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:479)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:464)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:464)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:454)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n  at scala.collection.immutable.List.foldLeft(List.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)\n  at org.apache.spark.sql.SnappySession$$anonfun$8.apply(SnappySession.scala:1132)\n  at org.apache.spark.sql.SnappySession$$anonfun$8.apply(SnappySession.scala:1132)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.sql.SnappySession.createTable(SnappySession.scala:1131)\n  at org.apache.spark.sql.execution.CreateMetastoreTableUsingSelect.run(ddl.scala:74)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.CodegenSparkFallback$$anonfun$doExecute$1.apply(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.CodegenSparkFallback$$anonfun$doExecute$1.apply(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.CodegenSparkFallback.executeWithFallback(CodegenSparkFallback.scala:72)\n  at org.apache.spark.sql.execution.CodegenSparkFallback.doExecute(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n  at org.apache.spark.sql.SnappySession$.evaluatePlan(SnappySession.scala:2117)\n  at org.apache.spark.sql.SnappySession$.sqlPlan(SnappySession.scala:2219)\n  at org.apache.spark.sql.SnappySession$$anonfun$sql$1.apply(SnappySession.scala:202)\n  at org.apache.spark.sql.SnappySession$$anonfun$sql$1.apply(SnappySession.scala:202)\n  at org.apache.spark.sql.aqp.SnappyContextFunctions.sql(SnappyContextFunctions.scala:110)\n  at org.apache.spark.sql.SnappySession.sql(SnappySession.scala:202)\n  ... 44 elided\n"}]},"apps":[],"jobName":"paragraph_1545223409098_1659340346","id":"20180523-195027_1337409592","dateCreated":"2018-12-19T18:13:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:46305"},{"text":"%snappydata\n","dateUpdated":"2018-12-19T18:32:03+0530","config":{"colWidth":12,"editorMode":"ace/mode/text","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false},"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545223409098_1659340346","id":"20181031-163754_315595751","dateCreated":"2018-12-19T18:13:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:46306"}],"name":"/data-sources/cassandra","id":"2DZYHNX1B","angularObjects":{"2DPH14A65:shared_process":[],"2DPGQSV7A:shared_process":[],"2DS3UYT9A:shared_process":[],"2DQJMX5DQ:shared_process":[],"2DQ81BPY5:shared_process":[],"2DS5QXNYN:existing_process":[],"2DQ7ZYMVJ:shared_process":[],"2DS5FRHTM:shared_process":[],"2DR9MGEP5:shared_process":[],"2DR5HDZ47:shared_process":[],"2DQJ6875T:shared_process":[],"2DPZS7MR2:shared_process":[],"2DQDKNSAD:shared_process":[],"2DSC5C8CE:shared_process":[],"2DPQT3RH5:shared_process":[],"2DS6XQYQV:shared_process":[],"2DP184Y6B:shared_process":[],"2DRQMH9ND:shared_process":[],"2DSHRMW36:shared_process":[],"2DR7P8QFZ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}