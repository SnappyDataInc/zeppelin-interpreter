{"paragraphs":[{"text":"%snappydata\n\nprintln(\"%html <h4> This notebook allows you to register external data sets sourced from files.</h4>\")\n\nprintln(\"%html <h4>We support the following sources: S3, HDFS, GCS, Azure, local file system and the following formats: CSV, Text, JSON, Parquet, ORC, XML and AVRO. </h4>\")","dateUpdated":"2018-05-31T09:21:45+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4> This notebook allows you to register external data sets sourced from files.</h4>\n"},{"type":"HTML","data":"<h4>We support the following sources: S3, HDFS, GCS, Azure, local file system and the following formats: CSV, Text, JSON, Parquet, ORC, XML and AVRO. </h4>\n"}]},"apps":[],"jobName":"paragraph_1527758284981_-880307601","id":"20180523-182306_1939739509","dateCreated":"2018-05-31T09:18:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:42202"},{"title":"STEP 1: Select the source of the files and the data format","text":"%snappydata\n\nprintln(\"%html <h4> Specify Dataset name(<b>registered as External table</b>), Source of files (S3, GCS, HDFS or local files), File format (CSV, JSON, Text..) </h4>\")\n\n//var dataFormat: Any = _\n// this curly brackets around block of code prevents all the REPL output from being displayed ...\n{\nval datasetName = z.input(\"Name the dataset being created:\", \"InvalidName\").asInstanceOf[String]\nval source = z.select(\"Select file source\", Seq((\"0\", \"None\"),\n                                    (\"1\",\"HDFS\"),\n                                    (\"2\",\"S3\"),\n                                    (\"3\",\"local Files\"),\n                                    (\"4\",\"GCS\"),\n                                    (\"5\",\"Azure Store\"))).asInstanceOf[String]\n\n  z.put(\"datasetName\", datasetName)\n  z.put(\"source\", source)\n  \n  val dataFormat = z.select(\"Data format\", Seq((\"CSV\",\"CSV\"),(\"Parquet\",\"Parquet\"),(\"JSON\",\"JSON\"), (\"ORC\",\"ORC\"), (\"AVRO\",\"AVRO\")))\n  z.put(\"dataFormat\", dataFormat)\nif (source != \"0\") {\n  z.run(\"20180507-214355_143278230\")\n}\n}\n","user":"anonymous","dateUpdated":"2018-06-04T11:42:14+0000","config":{"tableHide":false,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"runOnSelectionChange":true,"title":true,"results":{},"enabled":true},"settings":{"params":{"day":"2","Select file source":"1","S3 access ID":"AKIAILHSQ3FINHV473RQ","S3 access secret":"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","S3 bucket location":"zeppelindemo/airportdata_csv","Name the dataset being created:":"Airline","Data format":"AVRO"},"forms":{"Name the dataset being created:":{"name":"Name the dataset being created:","displayName":"Name the dataset being created:","type":"input","defaultValue":"InvalidName","hidden":false,"$$hashKey":"object:42442"},"Select file source":{"name":"Select file source","displayName":"Select file source","type":"select","defaultValue":"","options":[{"value":"0","displayName":"None","$$hashKey":"object:42461"},{"value":"1","displayName":"HDFS","$$hashKey":"object:42462"},{"value":"2","displayName":"S3","$$hashKey":"object:42463"},{"value":"3","displayName":"local Files","$$hashKey":"object:42464"},{"value":"4","displayName":"GCS","$$hashKey":"object:42465"},{"value":"5","displayName":"Azure Store","$$hashKey":"object:42466"}],"hidden":false,"$$hashKey":"object:42443"},"Data format":{"name":"Data format","displayName":"Data format","type":"select","defaultValue":"","options":[{"value":"CSV","displayName":"CSV","$$hashKey":"object:42471"},{"value":"Parquet","displayName":"Parquet","$$hashKey":"object:42472"},{"value":"JSON","displayName":"JSON","$$hashKey":"object:42473"},{"value":"ORC","displayName":"ORC","$$hashKey":"object:42474"},{"value":"AVRO","displayName":"AVRO","$$hashKey":"object:42475"}],"hidden":false,"$$hashKey":"object:42444"}}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4> Specify Dataset name(<b>registered as External table</b>), Source of files (S3, GCS, HDFS or local files), File format (CSV, JSON, Text..) </h4>\n"}]},"apps":[],"jobName":"paragraph_1527758284981_-880307601","id":"20180507-215201_1754631266","dateCreated":"2018-05-31T09:18:04+0000","dateStarted":"2018-06-04T11:42:14+0000","dateFinished":"2018-06-04T11:42:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:42203"},{"title":"STEP 2: Supply access credentials, options for Access","text":"%snappydata\n\n{\nimport org.apache.spark.sql._\n\n//println(\"%html <h4> Add instructions here ... </h4>\")\n\nvar df: DataFrame = null\nval source = z.get(\"source\")\n\nif (source == \"1\") {\n  val id = z.input(\"HDFS name node\", \"hadoop-hadoop-hdfs-nn:9000\")\n  val filePath = z.input(\"File name (path)\", \"/AirlineDataWithRowID\")\n  \n  if(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n\n  val path = s\"hdfs://$id$filePath\"\n  z.put(\"path\", s\"$path\") \n  \n  println(s\"HDFS path = $path\")\n  \n} else if (source == \"2\") {\n  val id = z.input(\"S3 access ID\",\"AKIAILHSQ3FINHV473RQ\")\n  val secret = z.input(\"S3 access secret\", \"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\")\n  val bucket = z.input(\"S3 bucket location\", \"zeppelindemo/airportdata\")\n  \n  if(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n  \n  val path = s\"s3a://$id:$secret@$bucket\"\n  z.put(\"path\", s\"$path\")\n  \n  println(s\"S3 path = $path\")\n  //z.run(\"20180507-221659_1464268700\")\n}\n\n def getCSVOptions(): Unit = {\n    z.put(\"csv_header\", z.input(\"Header in CSV file\", \"true\").asInstanceOf[String])\n    z.put(\"inferSchema\", z.input(\"Infer schema\", \"true\").asInstanceOf[String])\n    z.put(\"mode\", z.select(\"Mode for bad records\", Seq((\"DROPMALFORMED\",\"DROPMALFORMED\"),(\"FAILFAST\",\"FAILFAST\"))).asInstanceOf[String])\n }\n}","user":"anonymous","dateUpdated":"2018-06-04T10:33:35+0000","config":{"tableHide":false,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"runOnSelectionChange":true,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{"S3 access ID":"AKIAILHSQ3FINHV473RQ","S3 access secret":"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","S3 bucket location":"zeppelindemo/AirlineDataWithRowID","Header in CSV file":"true","Infer schema":"true","Mode for bad records":"DROPMALFORMED","HDFS name node":"hadoop-hadoop-hdfs-nn:9000","File name (path)":"/AirlineDataWithRowID_avro"},"forms":{"HDFS name node":{"name":"HDFS name node","displayName":"HDFS name node","type":"input","defaultValue":"hadoop-hadoop-hdfs-nn:9000","hidden":false,"$$hashKey":"object:42521"},"File name (path)":{"name":"File name (path)","displayName":"File name (path)","type":"input","defaultValue":"/AirlineDataWithRowID","hidden":false,"$$hashKey":"object:42522"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"HDFS path = hdfs://hadoop-hadoop-hdfs-nn:9000/AirlineDataWithRowID_avro\n"}]},"apps":[],"jobName":"paragraph_1527758284986_-880692350","id":"20180507-214355_143278230","dateCreated":"2018-05-31T09:18:04+0000","dateStarted":"2018-06-04T11:42:15+0000","dateFinished":"2018-06-04T11:42:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:42204"},{"title":"STEP 3: Connect to source, Infer Schema, register in SnappyData Catalog as External table","text":"%snappydata\n\n{\n  import com.databricks.spark.avro._\n  import org.apache.spark.sql._\n  \n\n  var df: DataFrame = null\n\n  val sns = new org.apache.spark.sql.SnappySession(sc)\n  val path = z.get(\"path\")\n  val dataFormat = z.get(\"dataFormat\")\n  val datasetName = z.get(\"datasetName\")\n  \n  sns.sql(s\"drop table if exists $datasetName\")\n  \n  dataFormat match {\n      case \"CSV\"  =>\n                    val csv_header = z.get(\"csv_header\")\n                    val inferSchema = z.get(\"inferSchema\")\n                    val mode = z.get(\"mode\")\n                    //println( s\"$csv_header , $inferSchema, $mode, $path\")\n                    sns.sql(s\"drop table if exists $datasetName\")\n                    sns.sql(s\"create external table $datasetName using csv options( header '$csv_header', inferSchema '$inferSchema', mode '$mode', path '$path')\" )\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n                    \n      case f@(\"Parquet\"| \"JSON\" | \"ORC\") => \n                    println(s\"Loading from path $path\")\n                    sns.sql(s\"create external table $datasetName using $f options( path '$path')\" )\n                    df = sns.table(s\"$datasetName\")\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n                    \n      case \"AVRO\" => \n                    println(s\"Loading from path $path\")\n                    sns.sql(s\"create external table $datasetName using com.databricks.spark.avro options( path '$path')\" )\n                    df = sns.table(s\"$datasetName\")\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n      case _ =>\n                    println(\"Data format not handled ...\")\n  }\n  \n}\n","user":"anonymous","dateUpdated":"2018-06-04T12:35:59+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Loading from path hdfs://hadoop-hadoop-hdfs-nn:9000/AirlineDataWithRowID_avro\n------- INFERED SCHEMA ------- \nroot\n |-- YEAR_: integer (nullable = true)\n |-- MONTH_: integer (nullable = true)\n |-- DAYOFMONTH: integer (nullable = true)\n |-- DAYOFWEEK: integer (nullable = true)\n |-- UNIQUECARRIER: string (nullable = true)\n |-- TAILNUM: string (nullable = true)\n |-- FLIGHTNUM: integer (nullable = true)\n |-- ORIGIN: string (nullable = true)\n |-- DEST: string (nullable = true)\n |-- CRSDEPTIME: integer (nullable = true)\n |-- DEPTIME: integer (nullable = true)\n |-- DEPDELAY: double (nullable = true)\n |-- TAXIOUT: double (nullable = true)\n |-- TAXIIN: double (nullable = true)\n |-- CRSARRTIME: string (nullable = true)\n |-- ARRTIME: string (nullable = true)\n |-- ARRDELAY: double (nullable = true)\n |-- CANCELLED: string (nullable = true)\n |-- CANCELLATIONCODE: string (nullable = true)\n |-- DIVERTED: double (nullable = true)\n |-- CRSELAPSEDTIME: double (nullable = true)\n |-- ACTUALELAPSEDTIME: double (nullable = true)\n |-- AIRTIME: double (nullable = true)\n |-- DISTANCE: double (nullable = true)\n |-- CARRIERDELAY: string (nullable = true)\n |-- WEATHERDELAY: string (nullable = true)\n |-- NASDELAY: string (nullable = true)\n |-- SECURITYDELAY: string (nullable = true)\n |-- LATEAIRCRAFTDELAY: string (nullable = true)\n |-- ARRDELAYSLOT: string (nullable = true)\n |-- FLIGHTIESTR: long (nullable = true)\n\n------- SAMPLE DATA  ------- \n"},{"type":"TABLE","data":"YEAR_\tMONTH_\tDAYOFMONTH\tDAYOFWEEK\tUNIQUECARRIER\tTAILNUM\tFLIGHTNUM\tORIGIN\tDEST\tCRSDEPTIME\tDEPTIME\tDEPDELAY\tTAXIOUT\tTAXIIN\tCRSARRTIME\tARRTIME\tARRDELAY\tCANCELLED\tCANCELLATIONCODE\tDIVERTED\tCRSELAPSEDTIME\tACTUALELAPSEDTIME\tAIRTIME\tDISTANCE\tCARRIERDELAY\tWEATHERDELAY\tNASDELAY\tSECURITYDELAY\tLATEAIRCRAFTDELAY\tARRDELAYSLOT\tFLIGHTIESTR\n2007\t4\t6\t5\tHA\tN486HA\t161\tOGG\tHNL\t1512\t1513\t1.0\t5.0\t6.0\t1546\t1550\t4.0\t0.00\t\t0.0\t34.0\t37.0\t26.0\t100.0\t\t\t\t\t\t\t8594525776\n2007\t4\t7\t6\tHA\tN481HA\t161\tOGG\tHNL\t1512\t1506\t-6.0\t8.0\t4.0\t1546\t1541\t-5.0\t0.00\t\t0.0\t34.0\t35.0\t23.0\t100.0\t\t\t\t\t\t\t8594525777\n2007\t4\t8\t7\tHA\tN478HA\t161\tOGG\tHNL\t1512\t1513\t1.0\t7.0\t6.0\t1546\t1548\t2.0\t0.00\t\t0.0\t34.0\t35.0\t22.0\t100.0\t\t\t\t\t\t\t8594525778\n2007\t4\t9\t1\tHA\tN481HA\t161\tOGG\tHNL\t1512\t1511\t-1.0\t9.0\t5.0\t1546\t1548\t2.0\t0.00\t\t0.0\t34.0\t37.0\t23.0\t100.0\t\t\t\t\t\t\t8594525779\n2007\t4\t10\t2\tHA\tN481HA\t161\tOGG\tHNL\t1512\t1511\t-1.0\t13.0\t4.0\t1546\t1550\t4.0\t0.00\t\t0.0\t34.0\t39.0\t22.0\t100.0\t\t\t\t\t\t\t8594525780\n2007\t4\t11\t3\tHA\tN480HA\t161\tOGG\tHNL\t1512\t1510\t-2.0\t7.0\t5.0\t1546\t1548\t2.0\t0.00\t\t0.0\t34.0\t38.0\t26.0\t100.0\t\t\t\t\t\t\t8594525781\n2007\t4\t12\t4\tHA\tN486HA\t161\tOGG\tHNL\t1512\t1512\t0.0\t7.0\t9.0\t1546\t1550\t4.0\t0.00\t\t0.0\t34.0\t38.0\t22.0\t100.0\t\t\t\t\t\t\t8594525782\n2007\t4\t13\t5\tHA\tN486HA\t161\tOGG\tHNL\t1512\t1511\t-1.0\t9.0\t9.0\t1546\t1550\t4.0\t0.00\t\t0.0\t34.0\t39.0\t21.0\t100.0\t\t\t\t\t\t\t8594525783\n2007\t4\t14\t6\tHA\tN484HA\t161\tOGG\tHNL\t1512\t1508\t-4.0\t10.0\t6.0\t1546\t1547\t1.0\t0.00\t\t0.0\t34.0\t39.0\t23.0\t100.0\t\t\t\t\t\t\t8594525784\n2007\t4\t15\t7\tHA\tN486HA\t161\tOGG\tHNL\t1512\t1510\t-2.0\t7.0\t6.0\t1546\t1543\t-3.0\t0.00\t\t0.0\t34.0\t33.0\t20.0\t100.0\t\t\t\t\t\t\t8594525785\n<!--TABLE_COMMENT-->\n<font color=red>Results are limited by 10.</font>"}]},"apps":[],"jobName":"paragraph_1527758284987_-881077098","id":"20180507-221659_1464268700","dateCreated":"2018-05-31T09:18:04+0000","dateStarted":"2018-06-04T11:44:06+0000","dateFinished":"2018-06-04T11:44:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:42205"},{"title":"STEP 4: Cache the external data in Snappy Column table","text":"%snappydata\n{\n  val sns = new org.apache.spark.sql.SnappySession(sc)\n  val datasetName = z.get(\"datasetName\")\n  val colTableName = datasetName+\"_cache\"\n  \n  sns.sql(s\"drop table if exists $colTableName\")\n  sns.sql(s\"create table $colTableName using column options(buckets '8') as (select * from $datasetName limit 20000000)\")\n  val df = sns.table(s\"$colTableName\")\n  println(s\"CREATED IN-MEMORY COLUMN TABLE $colTableName with row count = \" + df.count )\n}\n","user":"anonymous","dateUpdated":"2018-06-04T11:44:16+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"CREATED IN-MEMORY COLUMN TABLE Airline_cache with row count = 20000000\n"}]},"apps":[],"jobName":"paragraph_1527758284987_-881077098","id":"20180523-185011_76116349","dateCreated":"2018-05-31T09:18:04+0000","dateStarted":"2018-06-04T11:44:16+0000","dateFinished":"2018-06-04T11:45:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:42206"},{"text":"%snappydata\n","user":"anonymous","dateUpdated":"2018-06-04T12:36:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/text","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false},"lineNumbers":false,"title":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1527758284987_-881077098","id":"20180523-193608_929081111","dateCreated":"2018-05-31T09:18:04+0000","dateStarted":"2018-06-04T11:38:43+0000","dateFinished":"2018-06-04T11:41:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:42207"},{"text":"%snappydata.sql\n","user":"anonymous","dateUpdated":"2018-05-31T10:16:59+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527761819610_234567709","id":"20180531-101659_1262628788","dateCreated":"2018-05-31T10:16:59+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:42208"}],"name":"/data-sources/load-from-files(HDFS, GCS, Azure...)","id":"2DGKSYW9T","angularObjects":{"2DHNE6S86:shared_process":[],"2DGVY74ZG:shared_process":[],"2DJ1Q7W6S:shared_process":[],"2DGGMDXFM:shared_process":[],"2DGAWC75U:shared_process":[],"2DEU1DCNS:shared_process":[],"2DED22K38:existing_process":[],"2DE6VJ9NP:shared_process":[],"2DFTEHVD5:shared_process":[],"2DFZPUABF:shared_process":[],"2DJ55MVAN:shared_process":[],"2DET5NR7W:shared_process":[],"2DFGYC6P1:shared_process":[],"2DHW6ZJ2C:shared_process":[],"2DFD4BNN1:shared_process":[],"2DGQ6MYWC:shared_process":[],"2DFGQ31FF:shared_process":[],"2DGUX9VPE:shared_process":[],"2DEMFNEVF:shared_process":[],"2DER9QDGB:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}