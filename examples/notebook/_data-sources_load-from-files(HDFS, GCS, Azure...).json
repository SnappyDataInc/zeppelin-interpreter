{"paragraphs":[{"text":"%snappydata\n\nprintln(\"%html <h4> This notebook allows you to register external data sets sourced from files.</h4>\")\n\nprintln(\"%html <h4>We support the following sources: S3, HDFS, GCS, Azure, local file system and the following formats: CSV, Text, JSON, Parquet, ORC, XML and AVRO. </h4>\")","user":"anonymous","dateUpdated":"2018-06-07T11:55:27+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4> This notebook allows you to register external data sets sourced from files.</h4>\n"},{"type":"HTML","data":"<h4>We support the following sources: S3, HDFS, GCS, Azure, local file system and the following formats: CSV, Text, JSON, Parquet, ORC, XML and AVRO. </h4>\n"}]},"apps":[],"jobName":"paragraph_1527758284981_-880307601","id":"20180523-182306_1939739509","dateCreated":"2018-05-31T09:18:04+0000","dateStarted":"2018-06-07T11:55:25+0000","dateFinished":"2018-06-07T11:55:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7421"},{"title":"STEP 1: Select the source of the files and the data format","text":"%snappydata\n\nprintln(\"%html <h4> Specify Dataset name(<b>registered as External table</b>), Source of files (S3, GCS, HDFS or local files), File format (CSV, JSON, Text..) </h4>\")\n\n//var dataFormat: Any = _\n// this curly brackets around block of code prevents all the REPL output from being displayed ...\n{\nval datasetName = z.input(\"Name the dataset being created:\", \"InvalidName\").asInstanceOf[String]\nval source = z.select(\"Select file source\", Seq((\"0\", \"None\"),\n                                    (\"1\",\"HDFS\"),\n                                    (\"2\",\"S3\"),\n                                    (\"3\",\"local Files\"),\n                                    (\"4\",\"GCS\"),\n                                    (\"5\",\"Azure Store\"))).asInstanceOf[String]\n\n  z.put(\"datasetName\", datasetName)\n  z.put(\"source\", source)\n  if(source == \"5\") { //azure\n     println(s\"\"\"%html\n     <div><h4> <span style=\"font-weight: bold;\"> Azure Store </h4><h5> Deploy required Azure jars (hadoop-azure*.jar and azure-storage*.jar) if not already done by using the 'Deploy Connectors' notebook. Search the required Azure packages in <style=\"font-weight: bold;\"> <a href=\"https://mvnrepository.com/\" target=\"_blank\"> https://mvnrepository.com/</a> Get the packages coordinates from here and use in the deploy paragraph 'Deploy Connectors' notebook. Its format is groupId:artifactId:version </h5></div>\n     \"\"\")\n  }\n  \n  val dataFormat = z.select(\"Data format\", Seq((\"CSV\",\"CSV\"),(\"Parquet\",\"Parquet\"),(\"JSON\",\"JSON\"), (\"ORC\",\"ORC\"), (\"AVRO\",\"AVRO\")))\n  if(dataFormat == \"AVRO\") {\n     println(s\"\"\"%html\n     <div><h4><span style=\"font-weight: bold;\"> Avro </h4><h5> Deploy Spark Avro package if not already done by using the 'Deploy Connectors' notebook. Search Spark package in <a href=\"https://spark-packages.org/\" target=\"_blank\"> spark-packages.org</a> </span> <span> Get the packages coordinates from here and use in the deploy paragraph 'Deploy Connectors' notebook. Its format is groupId:artifactId:version </h5></span></div>\n     \"\"\")\n  }\n  \n  z.put(\"dataFormat\", dataFormat)\nif (source != \"0\") {\n  z.run(\"20180507-214355_143278230\")\n}\n}\n","user":"anonymous","dateUpdated":"2018-06-12T12:01:44+0000","config":{"tableHide":false,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"runOnSelectionChange":true,"title":true,"results":{},"enabled":true},"settings":{"params":{"day":"2","Select file source":"5","S3 access ID":"AKIAILHSQ3FINHV473RQ","S3 access secret":"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","S3 bucket location":"zeppelindemo/airportdata_csv","Name the dataset being created:":"Airline","Data format":"Parquet"},"forms":{"Name the dataset being created:":{"name":"Name the dataset being created:","displayName":"Name the dataset being created:","type":"input","defaultValue":"InvalidName","hidden":false,"$$hashKey":"object:8513"},"Select file source":{"name":"Select file source","displayName":"Select file source","type":"select","defaultValue":"","options":[{"value":"0","displayName":"None","$$hashKey":"object:8531"},{"value":"1","displayName":"HDFS","$$hashKey":"object:8532"},{"value":"2","displayName":"S3","$$hashKey":"object:8533"},{"value":"3","displayName":"local Files","$$hashKey":"object:8534"},{"value":"4","displayName":"GCS","$$hashKey":"object:8535"},{"value":"5","displayName":"Azure Store","$$hashKey":"object:8536"}],"hidden":false,"$$hashKey":"object:8514"},"Data format":{"name":"Data format","displayName":"Data format","type":"select","defaultValue":"","options":[{"value":"CSV","displayName":"CSV","$$hashKey":"object:8541"},{"value":"Parquet","displayName":"Parquet","$$hashKey":"object:8542"},{"value":"JSON","displayName":"JSON","$$hashKey":"object:8543"},{"value":"ORC","displayName":"ORC","$$hashKey":"object:8544"},{"value":"AVRO","displayName":"AVRO","$$hashKey":"object:8545"}],"hidden":false,"$$hashKey":"object:8515"}}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4> Specify Dataset name(<b>registered as External table</b>), Source of files (S3, GCS, HDFS or local files), File format (CSV, JSON, Text..) </h4>\n"},{"type":"HTML","data":"     <div><h4> <span style=\"font-weight: bold;\"> Azure Store </h4><h5> Deploy required Azure jars (hadoop-azure*.jar and azure-storage*.jar) if not already done by using the 'Deploy Connectors' notebook. Search the required Azure packages in <style=\"font-weight: bold;\"> <a href=\"https://mvnrepository.com/\" target=\"_blank\"> https://mvnrepository.com/</a> Get the packages coordinates from here and use in the deploy paragraph 'Deploy Connectors' notebook. Its format is groupId:artifactId:version </h5></div>\n     \n"}]},"apps":[],"jobName":"paragraph_1527758284981_-880307601","id":"20180507-215201_1754631266","dateCreated":"2018-05-31T09:18:04+0000","dateStarted":"2018-06-12T12:01:44+0000","dateFinished":"2018-06-12T12:01:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7422"},{"title":"STEP 2: Supply access credentials, options for Access","text":"%snappydata\n\n{\nimport org.apache.spark.sql._\n\n//println(\"%html <h4> Add instructions here ... </h4>\")\n\nvar df: DataFrame = null\nval source = z.get(\"source\")\n\nif (source == \"1\") {\n  val id = z.input(\"HDFS name node\", \"hadoop-hadoop-hdfs-nn:9000\")\n  val filePath = z.input(\"File name (path)\", \"/AirlineDataWithRowID\")\n  \n  if(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n\n  val path = s\"hdfs://$id$filePath\"\n  z.put(\"path\", s\"$path\") \n  \n  println(s\"HDFS path = $path\")\n  \n} else if (source == \"2\") {\n  val id = z.input(\"S3 access ID\",\"AKIAILHSQ3FINHV473RQ\")\n  val secret = z.input(\"S3 access secret\", \"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\")\n  val bucket = z.input(\"S3 bucket location\", \"zeppelindemo/airportdata\")\n  \n  if(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n  \n  val path = s\"s3a://$id:$secret@$bucket\"\n  z.put(\"path\", s\"$path\")\n  \n  println(s\"S3 path = $path\")\n  //z.run(\"20180507-221659_1464268700\")\n} else if (source == \"5\") {\n  val storageAccount = z.input(\"Azure storage account\",\"shirishd\")\n  val container = z.input(\"Azure container\", \"data\")\n  val key = z.input(\"Azure key\", \"\")\n  val filePath = z.input(\"Azure file path in container\", \"\")\n  \n  if(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n  \n  val path = s\"wasb://$container@$storageAccount.blob.core.windows.net/$filePath\"\n  z.put(\"path\", s\"$path\")\n  \n  println(s\"Azure path = $path\")\n  \n  sc.hadoopConfiguration.set(\"fs.wasb.impl\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n  sc.hadoopConfiguration.set(\"fs.AbstractFileSystem.wasb.impl\", \"org.apache.hadoop.fs.azure.Wasb\")\n  sc.hadoopConfiguration.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n  sc.hadoopConfiguration.set(\"fs.azure.account.key.shirishd.blob.core.windows.net\", s\"$key\")\n  \n  //z.run(\"20180507-221659_1464268700\")\n}\n\n def getCSVOptions(): Unit = {\n    z.put(\"csv_header\", z.input(\"Header in CSV file\", \"true\").asInstanceOf[String])\n    z.put(\"inferSchema\", z.input(\"Infer schema\", \"true\").asInstanceOf[String])\n    z.put(\"mode\", z.select(\"Mode for bad records\", Seq((\"DROPMALFORMED\",\"DROPMALFORMED\"),(\"FAILFAST\",\"FAILFAST\"))).asInstanceOf[String])\n }\n}","user":"anonymous","dateUpdated":"2018-06-07T12:08:49+0000","config":{"tableHide":false,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"runOnSelectionChange":true,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{"S3 access ID":"AKIAILHSQ3FINHV473RQ","S3 access secret":"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","S3 bucket location":"zeppelindemo/AirlineDataWithRowID","Header in CSV file":"true","Infer schema":"true","Mode for bad records":"DROPMALFORMED","HDFS name node":"hadoop-hadoop-hdfs-nn:9000","File name (path)":"/AirlineDataWithRowID_avro","Azure storage account":"shirishd","Azure container":"data","Azure key":"xxxx","Azure file path in container":"AirlineDataWithRowID"},"forms":{"Azure storage account":{"name":"Azure storage account","displayName":"Azure storage account","type":"input","defaultValue":"shirishd","hidden":false,"$$hashKey":"object:8562"},"Azure container":{"name":"Azure container","displayName":"Azure container","type":"input","defaultValue":"data","hidden":false,"$$hashKey":"object:8563"},"Azure key":{"name":"Azure key","displayName":"Azure key","type":"input","defaultValue":"","hidden":false,"$$hashKey":"object:8564"},"Azure file path in container":{"name":"Azure file path in container","displayName":"Azure file path in container","type":"input","defaultValue":"","hidden":false,"$$hashKey":"object:8565"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Azure path = wasb://data@shirishd.blob.core.windows.net/AirlineDataWithRowID\n"}]},"apps":[],"jobName":"paragraph_1527758284986_-880692350","id":"20180507-214355_143278230","dateCreated":"2018-05-31T09:18:04+0000","dateStarted":"2018-06-12T12:01:45+0000","dateFinished":"2018-06-12T12:01:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7423"},{"title":"STEP 3: Connect to source, Infer Schema, register in SnappyData Catalog as External table","text":"%snappydata\n\n{\n  import org.apache.spark.sql._\n  \n\n  var df: DataFrame = null\n\n  val sns = new org.apache.spark.sql.SnappySession(sc)\n  val path = z.get(\"path\")\n  val dataFormat = z.get(\"dataFormat\")\n  val datasetName = z.get(\"datasetName\")\n  \n  sns.sql(s\"drop table if exists $datasetName\")\n  \n  dataFormat match {\n      case \"CSV\"  =>\n                    val csv_header = z.get(\"csv_header\")\n                    val inferSchema = z.get(\"inferSchema\")\n                    val mode = z.get(\"mode\")\n                    //println( s\"$csv_header , $inferSchema, $mode, $path\")\n                    sns.sql(s\"drop table if exists $datasetName\")\n                    sns.sql(s\"create external table $datasetName using csv options( header '$csv_header', inferSchema '$inferSchema', mode '$mode', path '$path')\" )\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n                    \n      case f@(\"Parquet\"| \"JSON\" | \"ORC\") => \n                    println(s\"Loading from path $path\")\n                    sns.sql(s\"create external table $datasetName using $f options( path '$path')\" )\n                    df = sns.table(s\"$datasetName\")\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n                    \n      case \"AVRO\" => \n                    println(s\"Loading from path $path\")\n                    sns.sql(s\"create external table $datasetName using com.databricks.spark.avro options( path '$path')\" )\n                    df = sns.table(s\"$datasetName\")\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n      case _ =>\n                    println(\"Data format not handled ...\")\n  }\n  \n}\n","user":"anonymous","dateUpdated":"2018-06-07T12:10:04+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\n  at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.retrieveMetadata(AzureNativeFileSystemStore.java:1930)\n  at org.apache.hadoop.fs.azure.NativeAzureFileSystem.getFileStatus(NativeAzureFileSystem.java:1592)\n  at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:381)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:370)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:344)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog$$anon$1.load(SnappyStoreHiveCatalog.scala:210)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog$$anon$1.load(SnappyStoreHiveCatalog.scala:182)\n  at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n  at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n  at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n  at com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n  at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n  at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n  at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4880)\n  at com.google.common.cache.LocalCache$LocalLoadingCache.apply(LocalCache.java:4898)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog.getCachedHiveTable(SnappyStoreHiveCatalog.scala:257)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog.lookupRelation(SnappyStoreHiveCatalog.scala:492)\n  at org.apache.spark.sql.SnappySession.dropTable(SnappySession.scala:1329)\n  at org.apache.spark.sql.execution.DropTableOrViewCommand.run(ddl.scala:89)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.CodegenSparkFallback$$anonfun$doExecute$1.apply(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.CodegenSparkFallback$$anonfun$doExecute$1.apply(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.CodegenSparkFallback.executeWithFallback(CodegenSparkFallback.scala:72)\n  at org.apache.spark.sql.execution.CodegenSparkFallback.doExecute(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n  at org.apache.spark.sql.SnappySession$.evaluatePlan(SnappySession.scala:1983)\n  at org.apache.spark.sql.SnappySession$.sqlPlan(SnappySession.scala:2084)\n  at org.apache.spark.sql.SnappySession$$anonfun$sql$1.apply(SnappySession.scala:199)\n  at org.apache.spark.sql.SnappySession$$anonfun$sql$1.apply(SnappySession.scala:199)\n  at org.apache.spark.sql.aqp.SnappyContextFunctions.sql(SnappyContextFunctions.scala:91)\n  at org.apache.spark.sql.SnappySession.sql(SnappySession.scala:199)\n  ... 44 elided\nCaused by: com.microsoft.azure.storage.StorageException: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\n  at com.microsoft.azure.storage.StorageException.translateFromHttpStatus(StorageException.java:202)\n  at com.microsoft.azure.storage.StorageException.translateException(StorageException.java:172)\n  at com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)\n  at com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:177)\n  at com.microsoft.azure.storage.blob.CloudBlobContainer.downloadAttributes(CloudBlobContainer.java:551)\n  at org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobContainerWrapperImpl.downloadAttributes(StorageInterfaceImpl.java:235)\n  at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.checkContainer(AzureNativeFileSystemStore.java:1111)\n  at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.retrieveMetadata(AzureNativeFileSystemStore.java:1846)\n  ... 96 more\n"}]},"apps":[],"jobName":"paragraph_1527758284987_-881077098","id":"20180507-221659_1464268700","dateCreated":"2018-05-31T09:18:04+0000","dateStarted":"2018-06-07T12:09:52+0000","dateFinished":"2018-06-07T12:09:56+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:7424"},{"title":"STEP 4: Cache the external data in Snappy Column table","text":"%snappydata\n{\n  val sns = new org.apache.spark.sql.SnappySession(sc)\n  val datasetName = z.get(\"datasetName\")\n  val colTableName = datasetName+\"_cache\"\n  \n  sns.sql(s\"drop table if exists $colTableName\")\n  sns.sql(s\"create table $colTableName using column options(buckets '8') as (select * from $datasetName limit 20000000)\")\n  val df = sns.table(s\"$colTableName\")\n  println(s\"CREATED IN-MEMORY COLUMN TABLE $colTableName with row count = \" + df.count )\n}\n","user":"anonymous","dateUpdated":"2018-06-04T11:44:16+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"CREATED IN-MEMORY COLUMN TABLE Airline_cache with row count = 20000000\n"}]},"apps":[],"jobName":"paragraph_1527758284987_-881077098","id":"20180523-185011_76116349","dateCreated":"2018-05-31T09:18:04+0000","dateStarted":"2018-06-04T11:44:16+0000","dateFinished":"2018-06-04T11:45:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7425"},{"text":"\n","user":"anonymous","dateUpdated":"2018-06-07T12:10:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/text","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false},"lineNumbers":false,"title":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1527758284987_-881077098","id":"20180523-193608_929081111","dateCreated":"2018-05-31T09:18:04+0000","dateStarted":"2018-06-05T12:43:26+0000","dateFinished":"2018-06-05T12:44:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7426"},{"text":"%snappydata.sql\n","user":"anonymous","dateUpdated":"2018-05-31T10:16:59+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527761819610_234567709","id":"20180531-101659_1262628788","dateCreated":"2018-05-31T10:16:59+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:7427"}],"name":"/data-sources/load-from-files(HDFS, GCS, Azure...)","id":"2DGKSYW9T","angularObjects":{"2DGPYYH1N:shared_process":[],"2DG48W36Y:shared_process":[],"2DHE12TDV:shared_process":[],"2DHCD9UXW:shared_process":[],"2DF85TAJP:existing_process":[],"2DF7AAFKU:shared_process":[],"2DFVMQVDB:shared_process":[],"2DGVGHE3D:shared_process":[],"2DFNKNEEN:shared_process":[],"2DHECHK6X:shared_process":[],"2DH2UWJV5:shared_process":[],"2DH62E6VH:shared_process":[],"2DH477C2S:shared_process":[],"2DF24QSRF:shared_process":[],"2DFF32EZW:shared_process":[],"2DHYFNXRJ:shared_process":[],"2DH3S6BMK:shared_process":[],"2DF134G9P:shared_process":[],"2DH6VCNMW:shared_process":[],"2DGHYKJFH:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}