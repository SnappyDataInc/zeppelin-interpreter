{"paragraphs":[{"text":"%snappydata\n\nprintln(\"%html <h4> This notebook allows you to register external data sets sourced from files.</h4>\")\n\nprintln(\"%html <h4>We support the following sources: S3, HDFS, GCS, Azure, local file system and the following formats: CSV, Text, JSON, Parquet, ORC, XML and AVRO. </h4>\")","user":"anonymous","dateUpdated":"2018-10-11T16:40:29+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4> This notebook allows you to register external data sets sourced from files.</h4>\n"},{"type":"HTML","data":"<h4>We support the following sources: S3, HDFS, GCS, Azure, local file system and the following formats: CSV, Text, JSON, Parquet, ORC, XML and AVRO. </h4>\n"}]},"apps":[],"jobName":"paragraph_1536573317757_-1177365267","id":"20180523-182306_1939739509","dateCreated":"2018-09-10T15:25:17+0530","dateStarted":"2018-10-11T16:40:30+0530","dateFinished":"2018-10-11T16:40:30+0530","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9451"},{"title":"STEP 1: Select the source of the files and the data format","text":"%snappydata\n\nprintln(\"%html <h4> Specify Dataset name(<b>registered as External table</b>), Source of files (S3, GCS, HDFS or local files), File format (CSV, JSON, Text..) </h4>\")\n\n//var dataFormat: Any = _\n// this curly brackets around block of code prevents all the REPL output from being displayed ...\n{\nval datasetName = z.input(\"Name the dataset being created:\", \"InvalidName\").asInstanceOf[String]\nval source = z.select(\"Select file source\", Seq((\"0\", \"None\"),\n                                    (\"1\",\"HDFS\"),\n                                    (\"2\",\"S3\"),\n                                    (\"3\",\"local Files\"),\n                                    (\"4\",\"GCS\"),\n                                    (\"5\",\"Azure Store\"))).asInstanceOf[String]\n\n  z.put(\"datasetName\", datasetName)\n  z.put(\"source\", source)\n  if(source == \"5\") { //azure\n     println(s\"\"\"%html\n     <div><h4> <span style=\"font-weight: bold;\"> Azure Store </h4><h5> Deploy required Azure jars (hadoop-azure*.jar and azure-storage*.jar) if not already done by using the 'Deploy Connectors' notebook. Search the required Azure packages in <style=\"font-weight: bold;\"> <a href=\"https://mvnrepository.com/\" target=\"_blank\"> https://mvnrepository.com/</a> Get the packages coordinates from here and use in the deploy paragraph 'Deploy Connectors' notebook. Its format is groupId:artifactId:version </h5></div>\n     \"\"\")\n  }\n  \n  val dataFormat = z.select(\"File format\", Seq((\"CSV\",\"CSV\"),(\"Parquet\",\"Parquet\"),(\"JSON\",\"JSON\"), (\"ORC\",\"ORC\"), (\"AVRO\",\"AVRO\")))\n  if(dataFormat == \"AVRO\") {\n     println(s\"\"\"%html\n     <div><h4><span style=\"font-weight: bold;\"> Avro </h4><h5> Deploy Spark Avro package if not already done by using the 'Deploy Connectors' notebook. Search Spark package in <a href=\"https://spark-packages.org/\" target=\"_blank\"> spark-packages.org</a> </span> <span> Get the packages coordinates from here and use in the deploy paragraph 'Deploy Connectors' notebook. Its format is groupId:artifactId:version </h5></span></div>\n     \"\"\")\n  }\n  \n  z.put(\"dataFormat\", dataFormat)\nif (source != \"0\") {\n  z.run(\"20180507-214355_143278230\")\n}\n}\n","user":"anonymous","dateUpdated":"2018-10-12T10:34:08+0530","config":{"tableHide":false,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":false,"runOnSelectionChange":true,"title":true,"results":{},"enabled":true},"settings":{"params":{"day":"2","Select file source":"5","S3 access ID":"AKIAILHSQ3FINHV473RQ","S3 access secret":"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","S3 bucket location":"zeppelindemo/airportdata_csv","Name the dataset being created:":"Airlin","Data format":"Parquet","File format":"Parquet"},"forms":{"Name the dataset being created:":{"name":"Name the dataset being created:","displayName":"Name the dataset being created:","type":"input","defaultValue":"InvalidName","hidden":false,"$$hashKey":"object:9715"},"Select file source":{"name":"Select file source","displayName":"Select file source","type":"select","defaultValue":"","options":[{"value":"0","displayName":"None","$$hashKey":"object:9734"},{"value":"1","displayName":"HDFS","$$hashKey":"object:9735"},{"value":"2","displayName":"S3","$$hashKey":"object:9736"},{"value":"3","displayName":"local Files","$$hashKey":"object:9737"},{"value":"4","displayName":"GCS","$$hashKey":"object:9738"},{"value":"5","displayName":"Azure Store","$$hashKey":"object:9739"}],"hidden":false,"$$hashKey":"object:9716"},"File format":{"name":"File format","displayName":"File format","type":"select","defaultValue":"","options":[{"value":"CSV","displayName":"CSV","$$hashKey":"object:9744"},{"value":"Parquet","displayName":"Parquet","$$hashKey":"object:9745"},{"value":"JSON","displayName":"JSON","$$hashKey":"object:9746"},{"value":"ORC","displayName":"ORC","$$hashKey":"object:9747"},{"value":"AVRO","displayName":"AVRO","$$hashKey":"object:9748"}],"hidden":false,"$$hashKey":"object:9717"}}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4> Specify Dataset name(<b>registered as External table</b>), Source of files (S3, GCS, HDFS or local files), File format (CSV, JSON, Text..) </h4>\n"},{"type":"HTML","data":"     <div><h4> <span style=\"font-weight: bold;\"> Azure Store </h4><h5> Deploy required Azure jars (hadoop-azure*.jar and azure-storage*.jar) if not already done by using the 'Deploy Connectors' notebook. Search the required Azure packages in <style=\"font-weight: bold;\"> <a href=\"https://mvnrepository.com/\" target=\"_blank\"> https://mvnrepository.com/</a> Get the packages coordinates from here and use in the deploy paragraph 'Deploy Connectors' notebook. Its format is groupId:artifactId:version </h5></div>\n     \n"}]},"apps":[],"jobName":"paragraph_1536573317759_-1176595769","id":"20180507-215201_1754631266","dateCreated":"2018-09-10T15:25:17+0530","dateStarted":"2018-10-12T10:34:08+0530","dateFinished":"2018-10-12T10:34:42+0530","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9452"},{"title":"STEP 2: Supply access credentials, options for Access","text":"%snappydata\n{\nimport org.apache.spark.sql._\n\n//println(\"%html <h4> Add instructions here ... </h4>\")\n\nvar df: DataFrame = null\nval source = z.get(\"source\")\n\nif (source == \"1\") {\n  val id = z.input(\"HDFS name node\", \"hadoop-hadoop-hdfs-nn:9000\")\n  val filePath = z.input(\"File name (path)\", \"/AirlineDataWithRowID\")\n  \n  if(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n\n  val path = s\"hdfs://$id$filePath\"\n  z.put(\"path\", s\"$path\") \n  \n  println(s\"HDFS path = $path\")\n  \n} else if (source == \"2\") {\n  val id = z.input(\"S3 access ID\",\"AKIAILHSQ3FINHV473RQ\")\n  val secret = z.input(\"S3 access secret\", \"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\")\n  val bucket = z.input(\"S3 bucket location\", \"zeppelindemo/airportdata\")\n  \n  if(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n  \n  val path = s\"s3a://$id:$secret@$bucket\"\n  z.put(\"path\", s\"$path\")\n  \n  println(s\"S3 path = $path\")\n  \n} else if (source == \"5\") {\n  val storageAccount = z.input(\"Azure storage account\",\"shirishd\")\n  val container = z.input(\"Azure container\", \"data\")\n  val key = z.input(\"Azure key\", \"\")\n  val filePath = z.input(\"Azure file path in container\", \"\")\n  \n  if(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n  \n  val path = s\"wasb://$container@$storageAccount.blob.core.windows.net/$filePath\"\n  z.put(\"path\", s\"$path\")\n  \n  println(s\"Azure path = $path\")\n  \n  sc.hadoopConfiguration.set(\"fs.wasb.impl\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n  sc.hadoopConfiguration.set(\"fs.AbstractFileSystem.wasb.impl\", \"org.apache.hadoop.fs.azure.Wasb\")\n  sc.hadoopConfiguration.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n  \n  var string1=\"fs.azure.account.key.\"\n  val string2=\".blob.core.windows.net\"\n  string1+=storageAccount\n  string1+=string2\n  println(s\"$string1\")\n  sc.hadoopConfiguration.set(s\"$string1\", s\"$key\")\n  println(s\"$key\")\n  \n}\n\n\n def getCSVOptions(): Unit = {\n    z.put(\"csv_header\", z.input(\"Header in CSV file\", \"true\").asInstanceOf[String])\n    z.put(\"inferSchema\", z.input(\"Infer schema\", \"true\").asInstanceOf[String])\n    z.put(\"mode\", z.select(\"Mode for bad records\", Seq((\"DROPMALFORMED\",\"DROPMALFORMED\"),(\"FAILFAST\",\"FAILFAST\"))).asInstanceOf[String])\n }\n}","user":"anonymous","dateUpdated":"2018-10-12T11:39:06+0530","config":{"tableHide":false,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":false,"runOnSelectionChange":true,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{"S3 access ID":"AKIAILHSQ3FINHV473RQ","S3 access secret":"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","S3 bucket location":"zeppelindemo/AirlineDataWithRowID","Header in CSV file":"true","Infer schema":"true","Mode for bad records":"DROPMALFORMED","HDFS name node":"localhost:9000","File name (path)":"/jsonData","Azure storage account":"shirishd","Azure container":"data","Azure key":"KimtTUdQSO75VL2GDt8yFdNMotPI7gdRsJhrVxidWSPBdQ3339VveEp7oUExsSKPQ3tdgdo0gqqWm23/GZk7Eg==","Azure file path in container":"AirlineDataWithRowID"},"forms":{"Azure storage account":{"name":"Azure storage account","displayName":"Azure storage account","type":"input","defaultValue":"shirishd","hidden":false,"$$hashKey":"object:9796"},"Azure container":{"name":"Azure container","displayName":"Azure container","type":"input","defaultValue":"data","hidden":false,"$$hashKey":"object:9797"},"Azure key":{"name":"Azure key","displayName":"Azure key","type":"input","defaultValue":"","hidden":false,"$$hashKey":"object:9798"},"Azure file path in container":{"name":"Azure file path in container","displayName":"Azure file path in container","type":"input","defaultValue":"","hidden":false,"$$hashKey":"object:9799"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Azure path = wasb://data@shirishd.blob.core.windows.net/AirlineDataWithRowID\nfs.azure.account.key.shirishd.blob.core.windows.net\nKimtTUdQSO75VL2GDt8yFdNMotPI7gdRsJhrVxidWSPBdQ3339VveEp7oUExsSKPQ3tdgdo0gqqWm23/GZk7Eg==\n"}]},"apps":[],"jobName":"paragraph_1536573317759_-1176595769","id":"20180507-214355_143278230","dateCreated":"2018-09-10T15:25:17+0530","dateStarted":"2018-10-12T11:36:33+0530","dateFinished":"2018-10-12T11:36:34+0530","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9453"},{"title":"STEP 3: Enter additional parameters,Connect to source, Infer Schema, register in SnappyData Catalog as External table","text":"%snappydata\n\n{\nimport org.apache.spark.sql._\n\nvar flag:Boolean=false\nvar key:String=\" \"\nvar value:String=\" \"\nval ss = new org.apache.spark.sql.SnappySession(snc.sparkContext)\nvar stringToConcat = \"\"\nvar map1:Map[String,String] = Map()\nval parameterString = z.input(\"Enter optional parameters in the form of <key>=<value> separated by comma (,)\", \"\").asInstanceOf[String]\nif(parameterString != \"\"){\nfor (returnValue <- parameterString.split(\",\"))\n{\n  for(ret <- returnValue.split(\"=\"))\n    {\n      if(flag==false)\n      {\n        key=ret\n        flag=true\n      }\n      else\n      {\n        value=ret\n        flag=false\n       }\n    }\n map1 += key -> value\n }\n    map1.keys.foreach{ j =>\n    var temp=j\n    var temp2=map1(j)\n    stringToConcat += \",\"+temp+\" \"+\"'\"+temp2+\"'\"\n}\n}\nelse{\nprintln(\"No parameters Added\")\n}\n\n  var df: DataFrame = null\n\n  val path = z.get(\"path\")\n  val dataFormat = z.get(\"dataFormat\")\n  val datasetName = z.get(\"datasetName\")\n  \n  ss.sql(s\"drop table if exists $datasetName\")\n  \n  dataFormat match {\n      case \"CSV\"  =>\n                    val csv_header = z.get(\"csv_header\")\n                    val inferSchema = z.get(\"inferSchema\")\n                    val mode = z.get(\"mode\")\n                    println( s\"$csv_header , $inferSchema, $mode, $path\")\n                    ss.sql(s\"drop table if exists $datasetName\")\n                    val connectionString=s\"create external table $datasetName using csv options( header '$csv_header', inferSchema '$inferSchema', mode '$mode', path '$path' \" \n                    val connectionString2=connectionString.concat(stringToConcat)\n                    val finalConnectionString=connectionString2.concat(\")\")\n                    ss.sql(finalConnectionString)\n                    \n                    println(\"------- INFERED SCHEMA ------- \")\n                    df = ss.table(s\"$datasetName\")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n                    \n      case f@(\"Parquet\"| \"JSON\" | \"ORC\") => \n                    println(s\"Loading from path $path\")\n                    val connectionString=s\"create external table $datasetName using $f options( path '$path' \"\n                    val connectionString2=connectionString.concat(stringToConcat)\n                    val finalConnectionString=connectionString2.concat(\")\")\n                    ss.sql(finalConnectionString)\n                    \n                    df = ss.table(s\"$datasetName\")\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n                    \n      case \"AVRO\" => \n                    println(s\"Loading from path $path\")\n                    val connectionString=s\"create external table $datasetName using com.databricks.spark.avro options( path '$path' \" \n                    val connectionString2=connectionString.concat(stringToConcat)\n                    val finalConnectionString=connectionString2.concat(\")\")\n                    ss.sql(finalConnectionString)\n                    \n                    df = ss.table(s\"$datasetName\")\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n      case _ =>\n                    println(\"Data format not handled ...\")\n  }\n  \n}\n","user":"anonymous","dateUpdated":"2018-10-12T11:36:44+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{"Enter optional parameters in the form of <key>=<value> separated by comma (,)":""},"forms":{"Enter optional parameters in the form of <key>=<value> separated by comma (,)":{"name":"Enter optional parameters in the form of <key>=<value> separated by comma (,)","displayName":"Enter optional parameters in the form of <key>=<value> separated by comma (,)","type":"input","defaultValue":"","hidden":false,"$$hashKey":"object:9922"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"No parameters Added\nLoading from path wasb://data@shirishd.blob.core.windows.net/AirlineDataWithRowID\n------- INFERED SCHEMA ------- \nroot\n |-- YEAR_: integer (nullable = true)\n |-- MONTH_: integer (nullable = true)\n |-- DAYOFMONTH: integer (nullable = true)\n |-- DAYOFWEEK: integer (nullable = true)\n |-- UNIQUECARRIER: string (nullable = true)\n |-- TAILNUM: string (nullable = true)\n |-- FLIGHTNUM: integer (nullable = true)\n |-- ORIGIN: string (nullable = true)\n |-- DEST: string (nullable = true)\n |-- CRSDEPTIME: integer (nullable = true)\n |-- DEPTIME: integer (nullable = true)\n |-- DEPDELAY: double (nullable = true)\n |-- TAXIOUT: double (nullable = true)\n |-- TAXIIN: double (nullable = true)\n |-- CRSARRTIME: string (nullable = true)\n |-- ARRTIME: string (nullable = true)\n |-- ARRDELAY: double (nullable = true)\n |-- CANCELLED: string (nullable = true)\n |-- CANCELLATIONCODE: string (nullable = true)\n |-- DIVERTED: double (nullable = true)\n |-- CRSELAPSEDTIME: double (nullable = true)\n |-- ACTUALELAPSEDTIME: double (nullable = true)\n |-- AIRTIME: double (nullable = true)\n |-- DISTANCE: double (nullable = true)\n |-- CARRIERDELAY: string (nullable = true)\n |-- WEATHERDELAY: string (nullable = true)\n |-- NASDELAY: string (nullable = true)\n |-- SECURITYDELAY: string (nullable = true)\n |-- LATEAIRCRAFTDELAY: string (nullable = true)\n |-- ARRDELAYSLOT: string (nullable = true)\n |-- FLIGHTIESTR: long (nullable = true)\n\n------- SAMPLE DATA  ------- \n"},{"type":"TABLE","data":"YEAR_\tMONTH_\tDAYOFMONTH\tDAYOFWEEK\tUNIQUECARRIER\tTAILNUM\tFLIGHTNUM\tORIGIN\tDEST\tCRSDEPTIME\tDEPTIME\tDEPDELAY\tTAXIOUT\tTAXIIN\tCRSARRTIME\tARRTIME\tARRDELAY\tCANCELLED\tCANCELLATIONCODE\tDIVERTED\tCRSELAPSEDTIME\tACTUALELAPSEDTIME\tAIRTIME\tDISTANCE\tCARRIERDELAY\tWEATHERDELAY\tNASDELAY\tSECURITYDELAY\tLATEAIRCRAFTDELAY\tARRDELAYSLOT\tFLIGHTIESTR\n2008\t7\t2\t3\tDL\tN967DL\t156\tMCO\tATL\t1925\t1924\t-1.0\t8.0\t9.0\t2059\t2047\t-12.0\t0.00\t\t0.0\t94.0\t83.0\t66.0\t403.0\t\t\t\t\t\t\t8601458548\n2008\t7\t2\t3\tDL\tN941DL\t157\tATL\tMCO\t1750\t1750\t0.0\t16.0\t10.0\t1930\t1923\t-7.0\t0.00\t\t0.0\t100.0\t93.0\t67.0\t403.0\t\t\t\t\t\t\t8601458549\n2008\t7\t2\t3\tDL\tN3755D\t160\tCVG\tJFK\t1505\t1505\t0.0\t12.0\t47.0\t1730\t1735\t5.0\t0.00\t\t0.0\t145.0\t150.0\t91.0\t589.0\t\t\t\t\t\t\t8601458550\n2008\t7\t2\t3\tDL\tN925DL\t161\tJFK\tCVG\t1710\t1748\t38.0\t49.0\t5.0\t1944\t2004\t20.0\t0.00\t\t0.0\t154.0\t136.0\t82.0\t589.0\t20.00\t0.00\t0.00\t0.00\t0.00\t\t8601458551\n2008\t7\t2\t3\tDL\tN638DL\t162\tSEA\tJFK\t1240\t1241\t1.0\t43.0\t13.0\t2115\t2126\t11.0\t0.00\t\t0.0\t335.0\t345.0\t289.0\t2421.0\t\t\t\t\t\t\t8601458552\n2008\t7\t2\t3\tDL\tN613DL\t163\tJFK\tSEA\t2000\t1958\t-2.0\t41.0\t8.0\t2349\t2342\t-7.0\t0.00\t\t0.0\t409.0\t404.0\t355.0\t2421.0\t\t\t\t\t\t\t8601458553\n2008\t7\t2\t3\tDL\tN707TW\t165\tJFK\tLAX\t1430\t1434\t4.0\t32.0\t12.0\t1730\t1732\t2.0\t0.00\t\t0.0\t360.0\t358.0\t314.0\t2475.0\t\t\t\t\t\t\t8601458554\n2008\t7\t2\t3\tDL\tN396DA\t166\tDEN\tJFK\t800\t753\t-7.0\t11.0\t24.0\t1354\t1401\t7.0\t0.00\t\t0.0\t234.0\t248.0\t213.0\t1626.0\t\t\t\t\t\t\t8601458555\n2008\t7\t2\t3\tDL\tN3747D\t167\tJFK\tDEN\t1910\t1911\t1.0\t36.0\t8.0\t2150\t2147\t-3.0\t0.00\t\t0.0\t280.0\t276.0\t232.0\t1626.0\t\t\t\t\t\t\t8601458556\n2008\t7\t2\t3\tDL\tN651DL\t170\tPDX\tSLC\t1300\t1254\t-6.0\t13.0\t7.0\t1550\t1541\t-9.0\t0.00\t\t0.0\t110.0\t107.0\t87.0\t630.0\t\t\t\t\t\t\t8601458557\n<!--TABLE_COMMENT-->\n<font color=red>Results are limited by 10.</font>"}]},"apps":[],"jobName":"paragraph_1536573317760_-1092335760","id":"20180507-221659_1464268700","dateCreated":"2018-09-10T15:25:17+0530","dateStarted":"2018-10-12T11:36:44+0530","dateFinished":"2018-10-12T11:45:47+0530","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9454"},{"title":"STEP 4: Cache the external data in Snappy Column table","text":"%snappydata\n{\n  val sns = new org.apache.spark.sql.SnappySession(sc)\n  val datasetName = z.get(\"datasetName\")\n  val colTableName = datasetName+\"_cache\"\n  \n  sns.sql(s\"drop table if exists $colTableName\")\n  sns.sql(s\"create table $colTableName using column options(buckets '8') as (select * from $datasetName limit 20000000)\")\n  val df = sns.table(s\"$colTableName\")\n  println(s\"CREATED IN-MEMORY COLUMN TABLE $colTableName with row count = \" + df.count )\n}\n","user":"anonymous","dateUpdated":"2018-10-11T12:51:03+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"CREATED IN-MEMORY COLUMN TABLE Airline_cache with row count = 2\n"}]},"apps":[],"jobName":"paragraph_1536573317760_-1092335760","id":"20180523-185011_76116349","dateCreated":"2018-09-10T15:25:17+0530","dateStarted":"2018-09-25T14:54:23+0530","dateFinished":"2018-09-25T14:54:27+0530","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9455"},{"text":"\n","dateUpdated":"2018-09-10T15:25:17+0530","config":{"lineNumbers":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1536573317760_-1092335760","id":"20180523-193608_929081111","dateCreated":"2018-09-10T15:25:17+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9456"},{"text":"%snappydata.sql\n","dateUpdated":"2018-09-10T15:25:17+0530","config":{"colWidth":12,"editorMode":"ace/mode/text","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1536573317760_-1092335760","id":"20180531-101659_1262628788","dateCreated":"2018-09-10T15:25:17+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9457"}],"name":"/data-sources/load-from-files(HDFS, GCS, Azure...)","id":"2DS1EBMY6","angularObjects":{"2DVCCD1HB:shared_process":[],"2DUWTCRCE:shared_process":[],"2DSJVNUZ4:shared_process":[],"2DRTGSZDH:shared_process":[],"2DTSY8F4K:existing_process":[],"2DUG15QFX:shared_process":[],"2DT1MSVXR:shared_process":[],"2DTZU52BJ:shared_process":[],"2DUDZ18JN:shared_process":[],"2DRUE5Y7F:shared_process":[],"2DTW97ZMS:shared_process":[],"2DV8HEQ23:shared_process":[],"2DRZEUU18:shared_process":[],"2DUYZKKZQ:shared_process":[],"2DV9PT6SU:shared_process":[],"2DUYQS9YB:shared_process":[],"2DSY63EG8:shared_process":[],"2DSM33QMM:shared_process":[],"2DV48T9P4:shared_process":[],"2DUGW2ZFP:shared_process":[],"2DRE2WNK1:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}