{"paragraphs":[{"text":"%snappydata\n\nprintln(\"%html <h4> This notebook allows you to register external data sets sourced from files.</h4>\")\n\nprintln(\"%html <h4>We support the following sources: S3, HDFS, Azure, local file system and the following formats: CSV, Text, JSON, Parquet, ORC, XML and AVRO. </h4>\")","dateUpdated":"2018-12-20T10:55:14+0530","config":{"tableHide":true,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4> This notebook allows you to register external data sets sourced from files.</h4>\n"},{"type":"HTML","data":"<h4>We support the following sources: S3, HDFS, Azure, local file system and the following formats: CSV, Text, JSON, Parquet, ORC, XML and AVRO. </h4>\n"}]},"apps":[],"jobName":"paragraph_1545283514079_-856547504","id":"20180523-182306_1939739509","dateCreated":"2018-12-20T10:55:14+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:48058"},{"title":"STEP 1: Select the source of the files and the data format","text":"%snappydata\n\nprintln(\"%html <h4> Specify Dataset name(<b>registered as External table</b>), Source of files (HDFS, S3, Azure Store or local files), File format (CSV, JSON, Text..) </h4>\")\n\n//var dataFormat: Any = _\n// this curly brackets around block of code prevents all the REPL output from being displayed ...\n{\nval datasetName = z.input(\"Name the dataset being created:\", \"InvalidName\").asInstanceOf[String]\nval source = z.select(\"Select file source\", Seq((\"1\",\"HDFS\"),\n                                    (\"2\",\"S3\"),\n                                    (\"3\",\"Azure Store\"),\n                                    (\"4\",\"File System\"),\n                                    (\"5\",\"GCP\"))).asInstanceOf[String]\n\n  z.put(\"datasetName\", datasetName)\n  z.put(\"source\", source)\n  if(source == \"3\") { //azure\n     println(s\"\"\"%html\n     <div><h4> <span style=\"font-weight: bold;\"> Azure Store </h4><h5> Deploy required Azure jars (hadoop-azure*.jar and azure-storage*.jar) if not already done by using the 'Deploy Connectors' notebook. Search the required Azure packages in <style=\"font-weight: bold;\"> <a href=\"https://mvnrepository.com/\" target=\"_blank\"> https://mvnrepository.com/</a> Get the packages coordinates from here and use in the deploy paragraph 'Deploy Connectors' notebook. Its format is groupId:artifactId:version </h5></div>\n     \"\"\")\n  }\n  if(source == \"5\") { //azure\n     println(s\"\"\"%html\n     <div><h4> <span style=\"font-weight: bold;\"> GCP </h4><h5> Deploy required GCS connector jars (gcs-connector-hadoop2-latest.jar) if not already done by using the 'Deploy Connectors' notebook. Search the required GCS connector in <style=\"font-weight: bold;\"> <a href=\"https://mvnrepository.com/artifact/com.google.cloud.bigdataoss/gcs-connector/hadoop2-1.9.10\"> https://mvnrepository.com/</a> Get the packages coordinates from here and use in the deploy paragraph 'Deploy Connectors' notebook. Its format is groupId:artifactId:version </h5></div>\n     \"\"\")\n  }\n  \n  val dataFormat = z.select(\"File format\", Seq((\"CSV\",\"CSV\"),(\"Parquet\",\"Parquet\"),(\"JSON\",\"JSON\"), (\"ORC\",\"ORC\"), (\"AVRO\",\"AVRO\"),(\"XML\",\"XML\"),(\"Text\",\"Text\")))\n  if(dataFormat == \"AVRO\") {\n     println(s\"\"\"%html\n     <div><h4><span style=\"font-weight: bold;\"> Avro </h4><h5> Deploy Spark Avro package if not already done by using the 'Deploy Connectors' notebook. Search Spark package in <a href=\"https://spark-packages.org/\" target=\"_blank\"> spark-packages.org</a> </span> <span> Get the packages coordinates from here and use in the deploy paragraph 'Deploy Connectors' notebook. Its format is groupId:artifactId:version </h5></span></div>\n     \"\"\")\n     }\n  else if(dataFormat == \"XML\") {\n     println(s\"\"\"%html\n     <div><h4><span style=\"font-weight: bold;\"> XML </h4><h5> Deploy Spark XML package if not already done by using the 'Deploy Connectors' notebook. Select spark and scala version in <a href=\"https://mvnrepository.com/artifact/com.databricks/spark-xml\" target=\"_blank\">Spark-XML package</a> </span> <span> and get the packages coordinates from here and use in the deploy paragraph 'Deploy Connectors' notebook. Its format is groupId:artifactId:version </h5></span></div>\n     \"\"\")\n  }\nz.put(\"dataFormat\", dataFormat)\n\nif (source != \"0\") {\n  z.run(\"20180507-214355_143278230\")\n}\n}\n","dateUpdated":"2018-12-20T10:55:14+0530","config":{"tableHide":true,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"runOnSelectionChange":true,"title":true,"results":{},"enabled":true},"settings":{"params":{"day":"2","Select file source":"4","S3 access ID":"AKIAILHSQ3FINHV473RQ","S3 access secret":"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","S3 bucket location":"zeppelindemo/airportdata_csv","Name the dataset being created:":"abc1","Data format":"Parquet","File format":"CSV"},"forms":{"Name the dataset being created:":{"name":"Name the dataset being created:","displayName":"Name the dataset being created:","type":"input","defaultValue":"InvalidName","hidden":false,"$$hashKey":"object:48268"},"Select file source":{"name":"Select file source","displayName":"Select file source","type":"select","defaultValue":"","options":[{"value":"1","displayName":"HDFS","$$hashKey":"object:48287"},{"value":"2","displayName":"S3","$$hashKey":"object:48288"},{"value":"3","displayName":"Azure Store","$$hashKey":"object:48289"},{"value":"4","displayName":"File System","$$hashKey":"object:48290"},{"value":"5","displayName":"GCP","$$hashKey":"object:48291"}],"hidden":false,"$$hashKey":"object:48269"},"File format":{"name":"File format","displayName":"File format","type":"select","defaultValue":"","options":[{"value":"CSV","displayName":"CSV","$$hashKey":"object:48296"},{"value":"Parquet","displayName":"Parquet","$$hashKey":"object:48297"},{"value":"JSON","displayName":"JSON","$$hashKey":"object:48298"},{"value":"ORC","displayName":"ORC","$$hashKey":"object:48299"},{"value":"AVRO","displayName":"AVRO","$$hashKey":"object:48300"},{"value":"XML","displayName":"XML","$$hashKey":"object:48301"},{"value":"Text","displayName":"Text","$$hashKey":"object:48302"}],"hidden":false,"$$hashKey":"object:48270"}}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4> Specify Dataset name(<b>registered as External table</b>), Source of files (HDFS, S3, Azure Store or local files), File format (CSV, JSON, Text..) </h4>\n"}]},"apps":[],"jobName":"paragraph_1545283514080_-870783213","id":"20180507-215201_1754631266","dateCreated":"2018-12-20T10:55:14+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:48059"},{"title":"STEP 2: Supply access credentials, options for Access","text":"%snappydata\n{\nimport org.apache.spark.sql._\nimport org.apache.hadoop.conf.Configuration._\nimport org.apache.hadoop.fs.FileSystem._\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.fs.permission.FsPermission;\nimport org.apache.hadoop.util.Progressable;\n\nvar df: DataFrame = null\nval source = z.get(\"source\")\n\nif (source == \"1\") {\n  val id = z.input(\"HDFS name node\", \"hadoop-hadoop-hdfs-nn:9000\")\n  val filePath = z.input(\"File name (path)\", \"/AirlineDataWithRowID\")\n  \n  if(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n\n  val path = s\"hdfs://$id$filePath\"\n  z.put(\"path\", s\"$path\") \n  \n  println(s\"HDFS path = $path\")\n  \n} else if (source == \"2\") {\n  val id = z.input(\"S3 access ID\",\"AKIAILHSQ3FINHV473RQ\")\n  val secret = z.input(\"S3 access secret\", \"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\")\n  val bucket = z.input(\"S3 bucket location\", \"zeppelindemo/airportdata\")\n  \n  if(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n  \n  val path = s\"s3a://$id:$secret@$bucket\"\n  z.put(\"path\", s\"$path\")\n  \n  println(s\"S3 path = $path\")\n  \n} \nelse if (source == \"3\") {\n  val storageAccount = z.input(\"Azure storage account\",\"shirishd\")\n  val container = z.input(\"Azure container\", \"data\")\n  val key = z.input(\"Azure key\", \"\")\n  val filePath = z.input(\"Azure file path in container\", \"\")\n  \n  if(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n  \n  val path = s\"wasb://$container@$storageAccount.blob.core.windows.net/$filePath\"\n  z.put(\"path\", s\"$path\")\n  \n  println(s\"Azure path = $path\")\n  \n  sc.hadoopConfiguration.set(\"fs.wasb.impl\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n  sc.hadoopConfiguration.set(\"fs.AbstractFileSystem.wasb.impl\", \"org.apache.hadoop.fs.azure.Wasb\")\n  sc.hadoopConfiguration.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n  \n  var string1=\"fs.azure.account.key.\"\n  val string2=\".blob.core.windows.net\"\n  string1+=storageAccount\n  string1+=string2\n  println(s\"$string1\")\n  sc.hadoopConfiguration.set(s\"$string1\", s\"$key\")\n  println(s\"$key\")\n}\nelse if (source == \"4\") {\n    val path = z.input(\"File name (path)\", \"/AirlineDataWithRowID\")\n    z.put(\"path\", s\"$path\")\n}\nelse if (source == \"5\") {\n  val projectId = z.input(\"Enter Project ID\",\"ancient-ship-221306\")\n  val keyFilePath=z.input(\"Enter Path of Key File\",\"\")\n  val bucketName = z.input(\"Enter BucketName\", \"ancient-ship-221306.appspot.com\")\n  val filePath = z.input(\"GCP file path in Bucket\", \"data_parquet/users.parquet\")\n  \nif(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\n  \nval conf = sc.hadoopConfiguration\nconf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\nconf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\nconf.set(\"fs.gs.project.id\", s\"$projectId\")  // actual project filled in\nconf.set(\"google.cloud.auth.service.account.enable\", \"true\")\nconf.set(\"google.cloud.auth.service.account.json.keyfile\", s\"$keyFilePath\")  // actual keyfile filled in\n\nval path = new Path(s\"gs://$bucketName/$filePath\")\nval fs = path.getFileSystem(conf)\n\nprintln(fs)\n  z.put(\"path\", s\"$path\")\n  \n}\nif(z.get(\"dataFormat\") == \"XML\") getXMLOptions()\nif(z.get(\"dataFormat\") == \"CSV\") getCSVOptions()\nif(z.get(\"dataFormat\") == \"Text\") getTextOptions()\n\ndef getCSVOptions(): Unit = {\n    z.put(\"csv_header\", z.select(\"Header in CSV file\", Seq((\"true\" ,\"true\"),\n                                                            (\"false\",\"false\"))).asInstanceOf[String])\n    z.put(\"Delimiter\", z.input(\"Delimiter in CSV file\", \",\").asInstanceOf[String])\n    z.put(\"Encoding\", z.input(\"Encoding in CSV file\", \"UTF-8\").asInstanceOf[String])\n    z.put(\"mode\", z.select(\"Mode for bad records\", Seq((\"DROPMALFORMED\",\"DROPMALFORMED\"),(\"FAILFAST\",\"FAILFAST\"))).asInstanceOf[String])\n }\ndef getTextOptions(): Unit = {\n    z.put(\"text_header\", z.input(\"Header in Text file\", \"true\").asInstanceOf[String])\n    z.put(\"text_Delimiter\", z.input(\"Delimiter in Text file\", \" \").asInstanceOf[String])\n}\ndef getXMLOptions(): Unit = {\n     z.put(\"rowTag\", z.input(\"Enter Row Tag of XML file\",\"breakfast_menu\").asInstanceOf[String])\n}\n}","dateUpdated":"2018-12-20T10:55:14+0530","config":{"tableHide":true,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"runOnSelectionChange":true,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{"S3 access ID":"AKIAILHSQ3FINHV473RQ","S3 access secret":"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","S3 bucket location":"zeppelindemo/AirlineDataWithRowID","Header in CSV file":"true","Infer schema":"false","Mode for bad records":"DROPMALFORMED","HDFS name node":"localhost:9000","File name (path)":"/home/bpatil/Documents/SyntheticPatientsData/patients.csv","Azure storage account":"shirishd","Azure container":"data","Azure key":"KimtTUdQSO75VL2GDt8yFdNMotPI7gdRsJhrVxidWSPBdQ3339VveEp7oUExsSKPQ3tdgdo0gqqWm23/GZk7Eg==","Azure file path in container":"AirlineDataWithRowID","Enter Row Tag":"breakfast_menu","Enter Row Tag of XML file":"breakfast_menu","GCP storage account":"abhijeet18","Enter Project ID":"mygcp-project112","Enter Path of Key File":"","Enter BucketName":"abhijeet-bucket","GCP file path in Bucket":"mydata/people.json","Delimiter in CSV file":",","Encoding in CSV file":"UTF-8","Header in Text file":"true","Delimiter in Text file":" "},"forms":{"File name (path)":{"name":"File name (path)","displayName":"File name (path)","type":"input","defaultValue":"/AirlineDataWithRowID","hidden":false,"$$hashKey":"object:48338"},"Header in CSV file":{"name":"Header in CSV file","displayName":"Header in CSV file","type":"select","defaultValue":"","options":[{"value":"true","displayName":"true","$$hashKey":"object:48363"},{"value":"false","displayName":"false","$$hashKey":"object:48364"}],"hidden":false,"$$hashKey":"object:48339"},"Delimiter in CSV file":{"name":"Delimiter in CSV file","displayName":"Delimiter in CSV file","type":"input","defaultValue":",","hidden":false,"$$hashKey":"object:48340"},"Encoding in CSV file":{"name":"Encoding in CSV file","displayName":"Encoding in CSV file","type":"input","defaultValue":"UTF-8","hidden":false,"$$hashKey":"object:48341"},"Mode for bad records":{"name":"Mode for bad records","displayName":"Mode for bad records","type":"select","defaultValue":"","options":[{"value":"DROPMALFORMED","displayName":"DROPMALFORMED","$$hashKey":"object:48379"},{"value":"FAILFAST","displayName":"FAILFAST","$$hashKey":"object:48380"}],"hidden":false,"$$hashKey":"object:48342"}}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1545283514080_-870783213","id":"20180507-214355_143278230","dateCreated":"2018-12-20T10:55:14+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:48060"},{"title":"STEP 3: Enter additional parameters,Connect to source, Infer Schema, register in SnappyData Catalog as External table","text":"%snappydata\n{\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.{SnappySession, SparkSession}\n\nvar flag:Boolean=false\nvar key:String=\" \"\nvar value:String=\" \"\nval ss = new org.apache.spark.sql.SnappySession(snc.sparkContext)\nvar stringToConcat = \"\"\nvar map1:Map[String,String] = Map()\nval parameterString = z.input(\"Enter optional parameters in the form of <key>=<value> separated by comma (,)\", \"\").asInstanceOf[String]\nif(parameterString != \"\"){\nfor (returnValue <- parameterString.split(\",\"))\n{\n  for(ret <- returnValue.split(\"=\"))\n    {\n      if(flag==false)\n      {\n        key=ret\n        flag=true\n      }\n      else\n      {\n        value=ret\n        flag=false\n       }\n    }\n map1 += key -> value\n }\n    map1.keys.foreach{ j =>\n    var temp=j\n    var temp2=map1(j)\n    stringToConcat += \",\"+temp+\" \"+\"'\"+temp2+\"'\"\n}\n}\nelse{\nprintln(\"No parameters Added\")\n}\n  var df: DataFrame = null\n\n  val path = z.get(\"path\")\n  val dataFormat = z.get(\"dataFormat\")\n  val datasetName = z.get(\"datasetName\")\n  \n  ss.sql(s\"drop table if exists $datasetName\")\n  \n  dataFormat match {\n      case \"CSV\"  =>\n                    val csv_header = z.get(\"csv_header\")\n                    val Delimiter=z.get(\"Delimiter\")\n                    val inferSchema = z.get(\"inferSchema\")\n                    val encoding=z.get(\"Encoding\")\n                    val mode = z.get(\"mode\")\n                    println( s\"$csv_header , $inferSchema, $mode, $path ,$encoding\")\n                    \n                    val dfToSample=ss.read.format(\"csv\"). option(\"header\",\"true\").load(s\"$path\")\n                    val sampledData=dfToSample.sample(false,0.001)\n                    sampledData.count()\n                    sampledData.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\") .save(\"/tmp/mydata.csv\")\n                    val dfToLoadSchema=ss.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\" , \"true\").load(\"/tmp/mydata.csv\")\n                    val schema=dfToLoadSchema.schema\n                    ss.sql(s\"drop table if exists $datasetName\")\n                    val df1=ss.read.format(\"csv\").option(\"delimiter\", s\"$Delimiter\").option(\"ESCAPE quote\", '\"'). option(\"header\", s\"$csv_header\").schema(schema).option(\"encoding\", s\"$encoding\").option(\"mode\" , s\"$mode\").load(s\"$path\")\n\n                    df1.registerTempTable(s\"$datasetName\")\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df = ss.table(s\"$datasetName\")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n                    \n      case f@(\"Parquet\"| \"ORC\") => \n                    println(s\"Loading from path $path\")\n                    val connectionString=s\"create external table $datasetName using $f options( path '$path' \"\n                    val connectionString2=connectionString.concat(stringToConcat)\n                    val finalConnectionString=connectionString2.concat(\")\")\n                    ss.sql(finalConnectionString)\n                    \n                    df = ss.table(s\"$datasetName\")\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n      case \"JSON\" =>\n                    val multiline=z.select(\"multiline\",Seq((\"false\",\"false\"),(\"true\",\"true\"))).asInstanceOf[String]\n                    println(s\"$multiline\")\n                    println(s\"Loading from path $path\")\n                    if(multiline==\"true\")\n                    {\n                        val jsonRDD = snc.sparkContext.wholeTextFiles(z.get(\"path\").asInstanceOf[String]).map(x=>x._2)\n                        val namesJson = ss.read.json(jsonRDD)\n                        namesJson.printSchema\n                        namesJson.show()\n                        val connectionString=s\"create external table $datasetName using json options( path '$path' \"\n                        val connectionString2=connectionString.concat(stringToConcat)\n                        val finalConnectionString=connectionString2.concat(\")\")\n                        ss.sql(finalConnectionString)\n                    }\n                    else \n                    {\n                        val connectionString=s\"create external table $datasetName using json options( path '$path' \"\n                        val connectionString2=connectionString.concat(stringToConcat)\n                        val finalConnectionString=connectionString2.concat(\")\")\n                        ss.sql(finalConnectionString)\n                    }\n                    df = ss.table(s\"$datasetName\")\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n                    \n                    \n      case \"AVRO\" => \n                    println(s\"Loading from path $path\")\n                    val connectionString=s\"create external table $datasetName using com.databricks.spark.avro options( path '$path' \" \n                    val connectionString2=connectionString.concat(stringToConcat)\n                    val finalConnectionString=connectionString2.concat(\")\")\n                    ss.sql(finalConnectionString)\n                    \n                    df = ss.table(s\"$datasetName\")\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n                    \n        case \"XML\" => \n                    println(s\"Loading from path $path\")\n                    val rowTag=z.get(\"rowTag\")\n                    val connectionString=s\"create external table $datasetName using com.databricks.spark.xml options( path '$path',rowTag '$rowTag' \" \n                    val connectionString2=connectionString.concat(stringToConcat)\n                    val finalConnectionString=connectionString2.concat(\")\")\n                    ss.sql(finalConnectionString)\n                    \n                    df = ss.table(s\"$datasetName\")\n                    println(\"------- INFERED SCHEMA ------- \")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)\n        \n        case \"Text\"  =>\n\n                    val text_header = z.get(\"text_header\")\n                    val Delimiter=z.get(\"text_Delimiter\").asInstanceOf[String]\n                    val inferSchema = \"true\"\n                    println( s\"$text_header , $inferSchema, $path\")\n                    ss.sql(s\"drop table if exists $datasetName\")\n                    val connectionString=s\"create external table $datasetName using csv options( header '$text_header', delimiter  '$Delimiter' ,inferSchema '$inferSchema', path '$path' \" \n                    val connectionString2=connectionString.concat(stringToConcat)\n                    val finalConnectionString=connectionString2.concat(\")\")\n                    ss.sql(finalConnectionString)\n                    \n                    println(\"------- INFERED SCHEMA ------- \")\n                    df = ss.table(s\"$datasetName\")\n                    df.printSchema\n                    println(\"------- SAMPLE DATA  ------- \")\n                    z.show(df,10)            \n      case _ =>\n                    println(\"Data format not handled ...\")\n  }\n  \n}\n","dateUpdated":"2018-12-20T10:55:14+0530","config":{"tableHide":true,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"runOnSelectionChange":true,"results":{"1":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{"Enter optional parameters in the form of <key>=<value> separated by comma (,)":"","Row Tag":"breakfast_menu","Enter Row Tag":"breakfast_menu","multiline":"true"},"forms":{"Enter optional parameters in the form of <key>=<value> separated by comma (,)":{"name":"Enter optional parameters in the form of <key>=<value> separated by comma (,)","displayName":"Enter optional parameters in the form of <key>=<value> separated by comma (,)","type":"input","defaultValue":"","hidden":false,"$$hashKey":"object:48472"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nNo parameters Added\ntrue , false, DROPMALFORMED, /home/bpatil/Documents/SyntheticPatientsData/patients.csv ,UTF-8\n------- INFERED SCHEMA ------- \nroot\n |-- ID: string (nullable = true)\n |-- BIRTHDATE: timestamp (nullable = true)\n |-- DEATHDATE: timestamp (nullable = true)\n |-- SSN: string (nullable = true)\n |-- DRIVERS: string (nullable = true)\n |-- PASSPORT: string (nullable = true)\n |-- PREFIX: string (nullable = true)\n |-- FIRST: string (nullable = true)\n |-- LAST: string (nullable = true)\n |-- SUFFIX: string (nullable = true)\n |-- MAIDEN: string (nullable = true)\n |-- MARITAL: string (nullable = true)\n |-- RACE: string (nullable = true)\n |-- ETHNICITY: string (nullable = true)\n |-- GENDER: string (nullable = true)\n |-- BIRTHPLACE: string (nullable = true)\n |-- ADDRESS: string (nullable = true)\n |-- CITY: string (nullable = true)\n |-- STATE: string (nullable = true)\n |-- ZIP: integer (nullable = true)\n\n------- SAMPLE DATA  ------- \n"},{"type":"TABLE","data":"ID\tBIRTHDATE\tDEATHDATE\tSSN\tDRIVERS\tPASSPORT\tPREFIX\tFIRST\tLAST\tSUFFIX\tMAIDEN\tMARITAL\tRACE\tETHNICITY\tGENDER\tBIRTHPLACE\tADDRESS\tCITY\tSTATE\tZIP\nbd797bac-3fe9-4289-922c-7ca568dfb89e\t2006-09-14 00:00:00.0\tnull\t999-65-9867\tnull\tnull\tnull\tGregory545\tOsinski784\tnull\tnull\tnull\twhite\tenglish\tF\tWorcester\t275 Hagenes Lock\tBelchertown\tMassachusetts\t1007\nf94f9ffe-9f4b-4a14-860f-4cc48cf8e8ed\t2007-05-24 00:00:00.0\tnull\t999-95-2580\tnull\tnull\tnull\tBerniece493\tDavis923\tnull\tnull\tnull\thispanic\tcentral_american\tF\tBoston\t266 Harber Bay Apt 80\tBoston\tMassachusetts\t2108\n5f1cef17-b06a-42e6-ac6b-7151dc2c7221\t2000-04-06 00:00:00.0\tnull\t999-43-1147\tS99990853\tnull\tMs.\tHolley125\tBerge125\tnull\tnull\tnull\twhite\titalian\tF\tLynn\t667 Mitchell Dale Suite 96\tHolyoke\tMassachusetts\t1040\ne5574ace-e448-4866-b258-94aafe3ae4c0\t1993-04-13 00:00:00.0\tnull\t999-18-1475\tS99969530\tX42429471X\tMr.\tDamion480\tRunolfsson901\tnull\tnull\tnull\twhite\titalian\tM\tPeabody\t560 Nienow Dam\tBrockton\tMassachusetts\t2301\n35407c8f-8f63-4b5f-ad0d-52726cf97ffc\t1985-05-24 00:00:00.0\tnull\t999-67-6996\tS99937771\tX78033243X\tMr.\tKennith515\tBrown30\tnull\tnull\tS\twhite\tpolish\tM\tSpringfield\t317 Trantow Road Apt 43\tBrockton\tMassachusetts\t2301\nea158f42-6e23-44e7-bb35-7a6a361cc41e\t2001-08-26 00:00:00.0\tnull\t999-29-4050\tS99917036\tnull\tnull\tVesta905\tLedner144\tnull\tnull\tnull\thispanic\tpuerto_rican\tF\tLowell\t208 Halvorson Branch Unit 71\tChelmsford\tMassachusetts\t1824\ncc61259b-99cc-42cc-9e14-de085c249187\t1973-10-06 00:00:00.0\tnull\t999-45-2431\tS99936346\tX71845779X\tMr.\tHans694\tBraun514\tnull\tnull\tM\twhite\tpolish\tM\tBurlington\t1085 Gleichner Ranch\tLynn\tMassachusetts\t1901\n4c9ef4da-7117-41e4-8d88-1e643ce07389\t1972-05-28 00:00:00.0\tnull\t999-98-4261\tS99915252\tX44711127X\tMrs.\tShanta449\tLakin515\tnull\tBreitenberg711\tM\twhite\titalian\tF\tDedham\t615 Armstrong Hollow Apt 74\tNorth Reading\tMassachusetts\t1864\na7123cad-bec8-4f7e-a556-54a6f9b2d409\t2018-04-23 00:00:00.0\tnull\t999-39-7781\tnull\tnull\tnull\tKristina583\tMarks830\tnull\tnull\tnull\twhite\tpolish\tF\tFall River\t437 Cassin Frontage road Unit 0\tLexington\tMassachusetts\t2420\ne9601724-5078-437a-9984-15e0d04c9470\t1966-10-17 00:00:00.0\tnull\t999-61-4473\tS99971253\tX76803388X\tMr.\tKeneth579\tJacobi462\tnull\tnull\tM\twhite\tfrench\tM\tSandwich\t715 Gaylord Plaza\tWestford\tMassachusetts\t1886\n<!--TABLE_COMMENT-->\n<font color=red>Results are limited by 10.</font>"}]},"apps":[],"jobName":"paragraph_1545283514081_-871167962","id":"20180507-221659_1464268700","dateCreated":"2018-12-20T10:55:14+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:48061"},{"title":"STEP 4: Cache the external data in Snappy Column table","text":"%snappydata\n{\n  val sns = new org.apache.spark.sql.SnappySession(sc)\n  val datasetName = z.get(\"datasetName\")\n  val colTableName = datasetName+\"_cache\"\n  val tableType = z.select(\"SnappyData Table type\", Seq((\"column\" ,\"Column\"),\n                                                    (\"row\",\"Row\"))).asInstanceOf[String]\n  sns.sql(s\"drop table if exists $colTableName\")\n  sns.sql(s\"create table $colTableName using $tableType options(buckets '8') as (select * from $datasetName limit 20000000)\")\n  val df = sns.table(s\"$colTableName\")\n  println(s\"CREATED IN-MEMORY COLUMN TABLE $colTableName with row count = \" + df.count )\n}\n","dateUpdated":"2018-12-20T10:55:14+0530","config":{"tableHide":true,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"runOnSelectionChange":true,"results":{},"enabled":true},"settings":{"params":{"SnappyData Table type":"row"},"forms":{"SnappyData Table type":{"name":"SnappyData Table type","displayName":"SnappyData Table type","type":"select","defaultValue":"","options":[{"value":"column","displayName":"Column","$$hashKey":"object:48534"},{"value":"row","displayName":"Row","$$hashKey":"object:48535"}],"hidden":false,"$$hashKey":"object:48526"}}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.sql.TableNotFoundException: Table 'APP.ABC1' not found;\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog.lookupRelation(SnappyStoreHiveCatalog.scala:682)\n  at org.apache.spark.sql.hive.SnappyStoreHiveCatalog.lookupRelation(SnappyStoreHiveCatalog.scala:701)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:457)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:479)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:464)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:464)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:454)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n  at scala.collection.immutable.List.foldLeft(List.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)\n  at org.apache.spark.sql.SnappySession$$anonfun$8.apply(SnappySession.scala:1132)\n  at org.apache.spark.sql.SnappySession$$anonfun$8.apply(SnappySession.scala:1132)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.sql.SnappySession.createTable(SnappySession.scala:1131)\n  at org.apache.spark.sql.execution.CreateMetastoreTableUsingSelect.run(ddl.scala:74)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.CodegenSparkFallback$$anonfun$doExecute$1.apply(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.CodegenSparkFallback$$anonfun$doExecute$1.apply(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.CodegenSparkFallback.executeWithFallback(CodegenSparkFallback.scala:72)\n  at org.apache.spark.sql.execution.CodegenSparkFallback.doExecute(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n  at org.apache.spark.sql.SnappySession$.evaluatePlan(SnappySession.scala:2117)\n  at org.apache.spark.sql.SnappySession$.sqlPlan(SnappySession.scala:2219)\n  at org.apache.spark.sql.SnappySession$$anonfun$sql$1.apply(SnappySession.scala:202)\n  at org.apache.spark.sql.SnappySession$$anonfun$sql$1.apply(SnappySession.scala:202)\n  at org.apache.spark.sql.aqp.SnappyContextFunctions.sql(SnappyContextFunctions.scala:110)\n  at org.apache.spark.sql.SnappySession.sql(SnappySession.scala:202)\n  ... 50 elided\n"}]},"apps":[],"jobName":"paragraph_1545283514081_-871167962","id":"20180523-185011_76116349","dateCreated":"2018-12-20T10:55:14+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:48062"},{"text":"%snappydata.sql\n","dateUpdated":"2018-12-20T10:55:14+0530","config":{"tableHide":true,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545283514081_-871167962","id":"20180531-101659_1262628788","dateCreated":"2018-12-20T10:55:14+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:48063"}],"name":"/data-sources/load-from-files(HDFS,GCS,Azure...)","id":"2DXP84E8U","angularObjects":{"2DPH14A65:shared_process":[],"2DPGQSV7A:shared_process":[],"2DS3UYT9A:shared_process":[],"2DQJMX5DQ:shared_process":[],"2DQ81BPY5:shared_process":[],"2DS5QXNYN:existing_process":[],"2DQ7ZYMVJ:shared_process":[],"2DS5FRHTM:shared_process":[],"2DR9MGEP5:shared_process":[],"2DR5HDZ47:shared_process":[],"2DQJ6875T:shared_process":[],"2DPZS7MR2:shared_process":[],"2DQDKNSAD:shared_process":[],"2DSC5C8CE:shared_process":[],"2DPQT3RH5:shared_process":[],"2DS6XQYQV:shared_process":[],"2DP184Y6B:shared_process":[],"2DRQMH9ND:shared_process":[],"2DSHRMW36:shared_process":[],"2DR7P8QFZ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}