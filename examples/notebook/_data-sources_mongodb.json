{"paragraphs":[{"text":"%spark\n\nprintln(\"%html <h3> Specify Dataset name(registered as External table), mandatory config - URI, Database, Collection to read from </h3>\")\nprintln(\"%html <h4> <a href='https://docs.mongodb.com/spark-connector/current/configuration/' target='_blank'>Read description of all config options</a> </h4>\")\n","dateUpdated":"2018-12-10T16:15:23+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3> Specify Dataset name(registered as External table), mandatory config - URI, Database, Collection to read from </h3>\n"},{"type":"HTML","data":"<h4> <a href='https://docs.mongodb.com/spark-connector/current/configuration/' target='_blank'>Read description of all config options</a> </h4>\n"}]},"apps":[],"jobName":"paragraph_1540983089164_1830022324","id":"20180523-005307_463881908","dateCreated":"2018-10-31T16:21:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:49254"},{"title":"STEP I: Register the dataset using below MongoDB access options","text":"%snappydata\n\n// this curly brackets around block of code prevents all the REPL output from being displayed ...\n{\nval datasetName = z.input(\"Name the dataset being created:\", \"AirlineRef2\").asInstanceOf[String]\nval uri = z.input(\"URI:\", \"\").asInstanceOf[String]\nval database = z.input(\"Database:\", \"testdb\").asInstanceOf[String]\nval collection = z.input(\"Collection:\", \"airlineRef\").asInstanceOf[String]\nval sampleSize = z.input(\"Sample size (schema inference):\", \"1000\").asInstanceOf[String]\nval partitioner = z.input(\"Partitioner:\", \"MongoDefaultPartitioner\")\n//TODO: Several other important options to be added later ....\n\nz.put(\"datasetName\", datasetName)\nz.put(\"uri\", uri)\nz.put(\"database\", database)\n\nz.put(\"collection\",collection)\nz.put(\"partitioner\" ,partitioner)\nz.put(\"sampleSize\",sampleSize)\n\nvar connectionString =s\"create external table $datasetName using com.mongodb.spark.sql options (uri '$uri', database '$database', collection '$collection'\"\nz.put(\"connectionString\",connectionString)\n\n}\n","dateUpdated":"2018-12-10T16:15:23+0530","config":{"tableHide":true,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{"Name the dataset being created:":"ds2","URI:":"mongodb://localhost:27017/","Database:":"test","Collection:":"student","Sample size (schema inference):":"1000","Partitioner:":"MongoDefaultPartitioner"},"forms":{"Name the dataset being created:":{"name":"Name the dataset being created:","displayName":"Name the dataset being created:","type":"input","defaultValue":"AirlineRef2","hidden":false,"$$hashKey":"object:49454"},"URI:":{"name":"URI:","displayName":"URI:","type":"input","defaultValue":"","hidden":false,"$$hashKey":"object:49455"},"Database:":{"name":"Database:","displayName":"Database:","type":"input","defaultValue":"testdb","hidden":false,"$$hashKey":"object:49456"},"Collection:":{"name":"Collection:","displayName":"Collection:","type":"input","defaultValue":"airlineRef","hidden":false,"$$hashKey":"object:49457"},"Sample size (schema inference):":{"name":"Sample size (schema inference):","displayName":"Sample size (schema inference):","type":"input","defaultValue":"1000","hidden":false,"$$hashKey":"object:49458"},"Partitioner:":{"name":"Partitioner:","displayName":"Partitioner:","type":"input","defaultValue":"MongoDefaultPartitioner","hidden":false,"$$hashKey":"object:49459"}}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1540983089175_1839641046","id":"20180523-001031_1009080999","dateCreated":"2018-10-31T16:21:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49255"},{"title":"STEP II : Enter additional parameters here...","text":"%snappydata\n{  \nvar flag:Boolean=false\nvar key:String=\" \"\nvar value:String=\" \"\nvar map1:Map[String,String] = Map()\nval datasetName =z.get(\"datasetName\").asInstanceOf[String]\nval collection=z.get(\"collection\").asInstanceOf[String]\nval uri=z.get(\"uri\").asInstanceOf[String]\nval database=z.get(\"database\").asInstanceOf[String]\nval ss = new org.apache.spark.sql.SnappySession(snc.sparkContext)\n\nval parameterString = z.input(\"Enter optional parameters in the form of <key>=<value> separated by comma (,)\", \"\").asInstanceOf[String]\n\nif(parameterString != \"\"){\nfor (returnValue <- parameterString.split(\",\"))\n{\n  for(ret <- returnValue.split(\"=\"))\n    {\n      if(flag==false)\n      {\n        key=ret\n        flag=true\n      }\n      else\n      {\n        value=ret\n        flag=false\n       }\n    }\n map1 += key -> value\n }\n    var stringToConcat = \"\"\n    map1.keys.foreach{ j =>\n    var temp=j\n    var temp2=map1(j)\n     stringToConcat += \",\"+temp+\" \"+\"'\"+temp2+\"'\"\n    }\n     \n\nss.sql(s\"drop table if exists $datasetName\")\n\nval connectionString=z.get(\"connectionString\").asInstanceOf[String]\nval connectionString2=connectionString.concat(stringToConcat)\nval finalConnectionString=connectionString2.concat(\")\")\nss.sql(finalConnectionString)\n\n}\nelse{\n println(\"No extra parameters added .\")\n println(\"\")\n ss.sql(s\"drop table if exists $datasetName\")\n ss.sql(s\"create external table $datasetName using com.mongodb.spark.sql options (uri '$uri', database '$database', collection '$collection')\")\n}\nprintln\nprintln(s\"External table named $datasetName registered.\")\nprintln(s\"====== Schema for $datasetName ======\")\nss.table(datasetName).printSchema\n\n}\n","dateUpdated":"2018-12-10T16:15:23+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"results":{},"enabled":true,"tableHide":true},"settings":{"params":{"Give list of parameters:":" ","Enter optional parameters in the form of <key>=<value> separated by comma (,)":""},"forms":{"Enter optional parameters in the form of <key>=<value> separated by comma (,)":{"name":"Enter optional parameters in the form of <key>=<value> separated by comma (,)","displayName":"Enter optional parameters in the form of <key>=<value> separated by comma (,)","type":"input","defaultValue":"","hidden":false,"$$hashKey":"object:49531"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"No extra parameters added .\n\n\nExternal table named ds2 registered.\n====== Schema for ds2 ======\nroot\n |-- Name: string (nullable = true)\n |-- _id: struct (nullable = true)\n |    |-- oid: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1540983089176_1837717302","id":"20180925-121101_81419136","dateCreated":"2018-10-31T16:21:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49256"},{"title":"STEP III : Lets take a look at some sample data","text":"%snappydata\nval df1 = z.get(\"datasetName\").asInstanceOf[String]\nval df2 = snc.table(df1)\ndf2.collect().foreach(println)","dateUpdated":"2018-12-10T16:15:23+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"results":{},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df1: String = ds2\ndf2: org.apache.spark.sql.DataFrame = [Name: string, _id: struct<oid: string>]\n[Abhi,[5baca7989f62e363444720ed]]\n[Kajal,[5baca8a49f62e363444720ee]]\n[Kajal,[5baca9219f62e363444720ef]]\n[Lina,[5baca9319f62e363444720f0]]\n[Bhagyashri,[5baca93b9f62e363444720f1]]\n"}]},"apps":[],"jobName":"paragraph_1540983089177_1837332553","id":"20180523-010008_189512033","dateCreated":"2018-10-31T16:21:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49257"},{"title":"STEP IV : Cache the data in SnappyData Column/Row table","text":"%snappydata\n{\nimport org.apache.spark.sql.SnappySession\n\nval ss = new SnappySession(snc.sparkContext)\n\nval datasetName = z.get(\"datasetName\")\nval tabName = datasetName+ \"_cache\"\n\nval tableType = z.input(\"SnappyData Table type\", \"column\")\n\nss.sql(s\"drop table if exists $tabName\")\nss.sql(s\"create table $tabName using row as (select * from $datasetName)\" )\n//ss.sql(s\"create table $tabName using $tableType as (select code, description from $datasetName)\" )\n\nprintln(s\"Created table named $tabName\")\nprintln(\"With row count = \" + ss.table(s\"$tabName\").count )\n}","dateUpdated":"2018-12-10T16:15:23+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"title":true,"results":{},"enabled":true,"tableHide":true},"settings":{"params":{"SnappyData Table type":"Column"},"forms":{"SnappyData Table type":{"name":"SnappyData Table type","displayName":"SnappyData Table type","type":"input","defaultValue":"column","hidden":false,"$$hashKey":"object:49619"}}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.sql.AnalysisException: cannot resolve 'CAST(`_ID` AS BINARY)' due to data type mismatch: cannot cast StructType(StructField(oid,StringType,true)) to BinaryType;;\n'Insert Relation[NAME#21,_ID#22] RowFormatRelation[APP.DS2_CACHE], OverwriteOptions(false,Map()), false\n+- 'Project [cast(NAME#17 as string) AS NAME#23, cast(_ID#18 as binary) AS _ID#24]\n   +- Project [Name#13 AS NAME#17, _id#14 AS _ID#18]\n      +- MarkerForCreateTableAsSelect\n         +- Project [Name#13, _id#14]\n            +- SubqueryAlias DS2\n               +- Relation[Name#13,_id#14] MongoRelation(MongoRDD[65] at RDD at MongoRDD.scala:53,Some(StructType(StructField(Name,StringType,true), StructField(_id,StructType(StructField(oid,StringType,true)),true))))\n\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:91)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:83)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:287)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:273)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:273)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:284)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:294)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:299)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.immutable.List.map(List.scala:285)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:299)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:304)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:273)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:83)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:76)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:127)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:127)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:76)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n  at org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:73)\n  at org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:72)\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:78)\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:78)\n  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)\n  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n  at org.apache.spark.sql.row.JDBCMutableRelation.insert(JDBCMutableRelation.scala:293)\n  at org.apache.spark.sql.sources.MutableRelationProvider.createRelation(MutableRelationProvider.scala:129)\n  at org.apache.spark.sql.sources.MutableRelationProvider.createRelation(MutableRelationProvider.scala:32)\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:518)\n  at org.apache.spark.sql.SnappySession.createTableAsSelect(SnappySession.scala:1248)\n  at org.apache.spark.sql.SnappySession.createTable(SnappySession.scala:1143)\n  at org.apache.spark.sql.execution.CreateMetastoreTableUsingSelect.run(ddl.scala:74)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.CodegenSparkFallback$$anonfun$doExecute$1.apply(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.CodegenSparkFallback$$anonfun$doExecute$1.apply(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.CodegenSparkFallback.executeWithFallback(CodegenSparkFallback.scala:72)\n  at org.apache.spark.sql.execution.CodegenSparkFallback.doExecute(CodegenSparkFallback.scala:105)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n  at org.apache.spark.sql.SnappySession$.evaluatePlan(SnappySession.scala:2117)\n  at org.apache.spark.sql.SnappySession$.sqlPlan(SnappySession.scala:2219)\n  at org.apache.spark.sql.SnappySession$$anonfun$sql$1.apply(SnappySession.scala:202)\n  at org.apache.spark.sql.SnappySession$$anonfun$sql$1.apply(SnappySession.scala:202)\n  at org.apache.spark.sql.execution.SnappyContextAQPFunctions.sql(SnappyContextAQPFunctions.scala:464)\n  at org.apache.spark.sql.SnappySession.sql(SnappySession.scala:202)\n  ... 44 elided\n"}]},"apps":[],"jobName":"paragraph_1540983089187_1822712095","id":"20180523-195027_1337409592","dateCreated":"2018-10-31T16:21:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49258"},{"text":"%snappydata\n","dateUpdated":"2018-12-10T16:15:23+0530","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1540983089189_1820403601","id":"20180529-182653_1393497555","dateCreated":"2018-10-31T16:21:29+0530","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49259"}],"name":"/data-sources/mongodb","id":"2DUFEXJNU","angularObjects":{"2DPH14A65:shared_process":[],"2DPGQSV7A:shared_process":[],"2DS3UYT9A:shared_process":[],"2DQJMX5DQ:shared_process":[],"2DQ81BPY5:shared_process":[],"2DS5QXNYN:existing_process":[],"2DQ7ZYMVJ:shared_process":[],"2DS5FRHTM:shared_process":[],"2DR9MGEP5:shared_process":[],"2DR5HDZ47:shared_process":[],"2DQJ6875T:shared_process":[],"2DPZS7MR2:shared_process":[],"2DQDKNSAD:shared_process":[],"2DSC5C8CE:shared_process":[],"2DPQT3RH5:shared_process":[],"2DS6XQYQV:shared_process":[],"2DP184Y6B:shared_process":[],"2DRQMH9ND:shared_process":[],"2DSHRMW36:shared_process":[],"2DR7P8QFZ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}